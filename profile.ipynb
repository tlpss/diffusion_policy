{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_policy.policy.diffusion_unet_image_policy import DiffusionUnetImagePolicy\n",
    "from diffusion_policy.model.vision.multi_image_obs_encoder import MultiImageObsEncoder\n",
    "from diffusion_policy.model.vision.timm_model import TimmRGBModel\n",
    "from diffusers.schedulers.scheduling_ddim import DDIMScheduler\n",
    "from diffusion_policy.dataset.real_image_dataset import RealImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer shape meta:\n",
      "{'obs': {'camera_0': {'shape': [3, 224, 224], 'type': 'rgb'}, 'camera_1': {'shape': [3, 224, 224], 'type': 'rgb'}, 'robot_eef_pose_6d_rot': {'shape': [9], 'type': 'low_dim'}, 'gripper_width': {'shape': [1], 'type': 'low_dim'}}, 'action': {'shape': [10]}}\n",
      "Acquiring lock on cache.\n",
      "Loading cached ReplayBuffer from Disk.\n",
      "Loaded!\n",
      "image_keys: ['camera_0', 'camera_1']\n"
     ]
    }
   ],
   "source": [
    "n_latency_steps = 0\n",
    "n_obs_steps = 2\n",
    "n_action_steps = 8\n",
    "horizon = 16\n",
    "\n",
    "shape_meta = {\n",
    "    \"obs\": {\n",
    "        \"camera_0\": {\n",
    "            \"shape\": [3, 224, 224],\n",
    "            \"type\": \"rgb\"\n",
    "        },\n",
    "        \"camera_1\": {\n",
    "            \"shape\": [3, 224, 224],\n",
    "            \"type\": \"rgb\"\n",
    "        },\n",
    "        \"robot_eef_pose_6d_rot\": {\n",
    "            \"shape\": [9],\n",
    "            \"type\": \"low_dim\"\n",
    "        },\n",
    "        \"gripper_width\": {\n",
    "            \"shape\": [1],\n",
    "            \"type\": \"low_dim\"\n",
    "        }\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"shape\": [10]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "shape_meta = OmegaConf.create(shape_meta)\n",
    "dataset_path = \"/home/tlips/Code/diffusion_policy/data/demo_place-cb\"\n",
    "from torchvision.transforms import Resize, RandomCrop, ColorJitter\n",
    "dataset = RealImageDataset(shape_meta=shape_meta, dataset_path=dataset_path, image_buffer_resolution=[240,240],\n",
    " horizon=horizon, pad_before=n_obs_steps-1+n_latency_steps, pad_after=n_action_steps-1, n_obs_steps=n_obs_steps, n_latency_steps=n_latency_steps, \n",
    "use_cache=True, seed=42, val_ratio=0.1, max_train_episodes=None,\n",
    " delta_action=True,\n",
    " image_transforms=[Resize((240,240)),RandomCrop((224,224)), ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x = dataset[0]\n",
    "print(x[\"obs\"][\"camera_0\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         14979 function calls (14965 primitive calls) in 0.027 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.027    0.027 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.027    0.027 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.027    0.027 real_image_dataset.py:287(__getitem__)\n",
      "        2    0.000    0.000    0.022    0.011 transforms.py:92(__call__)\n",
      "        6    0.000    0.000    0.022    0.004 module.py:1124(_call_impl)\n",
      "        2    0.000    0.000    0.020    0.010 transforms.py:1225(forward)\n",
      "        2    0.000    0.000    0.018    0.009 functional.py:866(adjust_hue)\n",
      "        2    0.000    0.000    0.018    0.009 functional_tensor.py:193(adjust_hue)\n",
      "        2    0.001    0.000    0.015    0.007 functional_tensor.py:301(_hsv2rgb)\n",
      "        2    0.000    0.000    0.012    0.006 functional.py:192(einsum)\n",
      "        2    0.012    0.006    0.012    0.006 {built-in method torch.einsum}\n",
      "        1    0.000    0.000    0.004    0.004 real_image_dataset.py:279(_get_from_sampler)\n",
      "        1    0.000    0.000    0.004    0.004 sampler.py:125(sample_sequence)\n",
      "        2    0.002    0.001    0.003    0.001 functional_tensor.py:262(_rgb2hsv)\n",
      "        5    0.000    0.000    0.003    0.001 core.py:648(__getitem__)\n",
      "        5    0.000    0.000    0.002    0.000 core.py:791(get_basic_selection)\n",
      "        5    0.000    0.000    0.002    0.000 core.py:951(_get_basic_selection_nd)\n",
      "        5    0.000    0.000    0.002    0.000 core.py:1219(_get_selection)\n",
      "       23    0.000    0.000    0.002    0.000 core.py:1906(_chunk_getitem)\n",
      "        2    0.000    0.000    0.002    0.001 transforms.py:341(forward)\n",
      "        2    0.000    0.000    0.002    0.001 functional.py:363(resize)\n",
      "        2    0.000    0.000    0.002    0.001 functional_tensor.py:429(resize)\n",
      "        2    0.000    0.000    0.002    0.001 functional.py:3757(interpolate)\n",
      "        2    0.002    0.001    0.002    0.001 {built-in method torch._C._nn.upsample_bilinear2d}\n",
      "     1067    0.000    0.000    0.002    0.000 {built-in method builtins.isinstance}\n",
      "       14    0.002    0.000    0.002    0.000 {built-in method torch.stack}\n",
      "       68    0.000    0.000    0.002    0.000 compat.py:13(ensure_ndarray_like)\n",
      "       68    0.000    0.000    0.001    0.000 ndarray_like.py:54(is_ndarray_like)\n",
      "       23    0.000    0.000    0.001    0.000 core.py:1823(_process_chunk)\n",
      "       68    0.000    0.000    0.001    0.000 typing.py:1141(__instancecheck__)\n",
      "       55    0.000    0.000    0.001    0.000 compat.py:48(ensure_ndarray)\n",
      "        6    0.001    0.000    0.001    0.000 functional_tensor.py:256(_blend)\n",
      "        2    0.000    0.000    0.001    0.001 functional.py:844(adjust_saturation)\n",
      "        2    0.000    0.000    0.001    0.000 functional_tensor.py:222(adjust_saturation)\n",
      "      136    0.001    0.000    0.001    0.000 typing.py:1065(_get_protocol_attrs)\n",
      "       13    0.000    0.000    0.001    0.000 compat.py:126(ensure_contiguous_ndarray)\n",
      "        2    0.000    0.000    0.001    0.000 functional.py:822(adjust_contrast)\n",
      "       68    0.000    0.000    0.001    0.000 typing.py:1082(_is_callable_members_only)\n",
      "        4    0.001    0.000    0.001    0.000 functional_tensor.py:145(rgb_to_grayscale)\n",
      "        5    0.000    0.000    0.001    0.000 replay_buffer.py:415(__getitem__)\n",
      "        2    0.000    0.000    0.001    0.000 functional_tensor.py:176(adjust_contrast)\n",
      "        5    0.000    0.000    0.001    0.000 hierarchy.py:388(__getitem__)\n",
      "      182    0.000    0.000    0.001    0.000 {built-in method builtins.all}\n",
      "        5    0.000    0.000    0.000    0.000 core.py:154(__init__)\n",
      "       16    0.000    0.000    0.000    0.000 {method 'to' of 'torch._C._TensorBase' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.max}\n",
      "        2    0.000    0.000    0.000    0.000 functional.py:800(adjust_brightness)\n",
      "        2    0.000    0.000    0.000    0.000 functional_tensor.py:165(adjust_brightness)\n",
      "        5    0.000    0.000    0.000    0.000 core.py:217(_load_metadata)\n",
      "        5    0.000    0.000    0.000    0.000 core.py:226(_load_metadata_nosync)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.min}\n",
      "        5    0.000    0.000    0.000    0.000 meta.py:108(decode_array_metadata)\n",
      "        2    0.000    0.000    0.000    0.000 listconfig.py:512(__eq__)\n",
      "        5    0.000    0.000    0.000    0.000 meta.py:90(parse_metadata)\n",
      "       13    0.000    0.000    0.000    0.000 compat.py:70(ensure_contiguous_ndarray_like)\n",
      "        5    0.000    0.000    0.000    0.000 util.py:54(json_loads)\n",
      "      656    0.000    0.000    0.000    0.000 typing.py:1149(<genexpr>)\n",
      "        5    0.000    0.000    0.000    0.000 compat.py:178(ensure_text)\n",
      "        2    0.000    0.000    0.000    0.000 listconfig.py:46(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 listconfig.py:610(_set_value)\n",
      "        2    0.000    0.000    0.000    0.000 listconfig.py:620(_set_value_impl)\n",
      "        2    0.000    0.000    0.000    0.000 real_image_dataset.py:35(position_and_rot6d_to_se3)\n",
      "       25    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "       23    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(copyto)\n",
      "        2    0.000    0.000    0.000    0.000 listconfig.py:293(append)\n",
      "        2    0.000    0.000    0.000    0.000 basecontainer.py:522(_set_item_impl)\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method numpy.zeros}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method torch.where}\n",
      "     3557    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
      "        4    0.000    0.000    0.000    0.000 numeric.py:289(full)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.fmod}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'clamp' of 'torch._C._TensorBase' objects}\n",
      "        2    0.000    0.000    0.000    0.000 rotation_conversions.py:556(rotation_6d_to_matrix)\n",
      "       28    0.000    0.000    0.000    0.000 indexing.py:356(__iter__)\n",
      "       23    0.000    0.000    0.000    0.000 indexing.py:312(is_contiguous_selection)\n",
      "       77    0.000    0.000    0.000    0.000 indexing.py:314(<genexpr>)\n",
      "        5    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "        8    0.000    0.000    0.000    0.000 _tensor.py:26(wrapped)\n",
      "        4    0.000    0.000    0.000    0.000 dictconfig.py:361(__getitem__)\n",
      "     1133    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        4    0.000    0.000    0.000    0.000 dictconfig.py:438(_get_impl)\n",
      "        8    0.000    0.000    0.000    0.000 _tensor.py:637(__rsub__)\n",
      "        5    0.000    0.000    0.000    0.000 indexing.py:328(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method torch.rsub}\n",
      "        2    0.000    0.000    0.000    0.000 transforms.py:655(forward)\n",
      "        2    0.000    0.000    0.000    0.000 basecontainer.py:620(_wrap_value_and_set)\n",
      "        4    0.000    0.000    0.000    0.000 functional.py:4598(normalize)\n",
      "        2    0.000    0.000    0.000    0.000 omegaconf.py:1076(_maybe_wrap)\n",
      "        2    0.000    0.000    0.000    0.000 omegaconf.py:979(_node_wrap)\n",
      "       54    0.000    0.000    0.000    0.000 indexing.py:36(is_integer_array)\n",
      "      262    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)\n",
      "        2    0.000    0.000    0.000    0.000 listconfig.py:662(_list_eq)\n",
      "     1904    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
      "        2    0.000    0.000    0.000    0.000 listconfig.py:90(_validate_set)\n",
      "       46    0.000    0.000    0.000    0.000 indexing.py:178(__iter__)\n",
      "        4    0.000    0.000    0.000    0.000 basecontainer.py:86(_resolve_with_default)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.mean}\n",
      "      729    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method torch.clamp}\n",
      "        2    0.000    0.000    0.000    0.000 transforms.py:1193(get_params)\n",
      "       32    0.000    0.000    0.000    0.000 _utils.py:507(_is_missing_value)\n",
      "        2    0.000    0.000    0.000    0.000 nodes.py:128(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.zeros_like}\n",
      "       54    0.000    0.000    0.000    0.000 numeric.py:1873(isscalar)\n",
      "        8    0.000    0.000    0.000    0.000 basecontainer.py:64(_get_child)\n",
      "       23    0.000    0.000    0.000    0.000 util.py:200(is_total_slice)\n",
      "       15    0.000    0.000    0.000    0.000 util.py:309(normalize_storage_path)\n",
      "       28    0.000    0.000    0.000    0.000 storage.py:816(__getitem__)\n",
      "       12    0.000    0.000    0.000    0.000 {method 'unbind' of 'torch._C._TensorBase' objects}\n",
      "        2    0.000    0.000    0.000    0.000 nodes.py:24(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 transforms.py:620(get_params)\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        4    0.000    0.000    0.000    0.000 _tensor.py:527(norm)\n",
      "        2    0.000    0.000    0.000    0.000 basecontainer.py:637(_item_eq)\n",
      "      262    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "        5    0.000    0.000    0.000    0.000 storage.py:101(contains_array)\n",
      "       16    0.000    0.000    0.000    0.000 utils.py:543(_log_api_usage_once)\n",
      "       10    0.000    0.000    0.000    0.000 _utils.py:448(is_structured_config)\n",
      "        4    0.000    0.000    0.000    0.000 functional.py:1345(norm)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch.matmul}\n",
      "      136    0.000    0.000    0.000    0.000 typing.py:1084(<genexpr>)\n",
      "  189/183    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        5    0.000    0.000    0.000    0.000 __init__.py:299(loads)\n",
      "        8    0.000    0.000    0.000    0.000 basecontainer.py:179(__len__)\n",
      "        8    0.000    0.000    0.000    0.000 _utils.py:747(_get_value)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'repeat' of 'torch._C._TensorBase' objects}\n",
      "       23    0.000    0.000    0.000    0.000 core.py:478(_cdata_shape)\n",
      "       10    0.000    0.000    0.000    0.000 _utils.py:540(get_value_kind)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.ones_like}\n",
      "       26    0.000    0.000    0.000    0.000 functional_tensor.py:24(get_dimensions)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch._C._linalg.linalg_inv}\n",
      "       26    0.000    0.000    0.000    0.000 _utils.py:566(_is_interpolation)\n",
      "        4    0.000    0.000    0.000    0.000 contextlib.py:114(__enter__)\n",
      "       33    0.000    0.000    0.000    0.000 storage.py:789(_get_parent)\n",
      "       18    0.000    0.000    0.000    0.000 base.py:298(_is_missing)\n",
      "       14    0.000    0.000    0.000    0.000 indexing.py:165(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 omegaconf.py:936(flag_override)\n",
      "        5    0.000    0.000    0.000    0.000 decoder.py:332(decode)\n",
      "        5    0.000    0.000    0.000    0.000 hierarchy.py:360(_item_path)\n",
      "        8    0.000    0.000    0.000    0.000 base.py:152(_set_flag)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.floor}\n",
      "       88    0.000    0.000    0.000    0.000 {built-in method numpy.array}\n",
      "        8    0.000    0.000    0.000    0.000 listconfig.py:402(_get_node)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method torch.randint}\n",
      "        4    0.000    0.000    0.000    0.000 dictconfig.py:455(_get_node)\n",
      "        2    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(moveaxis)\n",
      "        5    0.000    0.000    0.000    0.000 storage.py:158(normalize_store_arg)\n",
      "        1    0.000    0.000    0.000    0.000 real_image_dataset.py:51(se3_to_position_and_rot6d)\n",
      "       68    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1033(_handle_fromlist)\n",
      "       12    0.000    0.000    0.000    0.000 functional_tensor.py:62(_assert_channels)\n",
      "      628    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
      "        2    0.000    0.000    0.000    0.000 omegaconf.py:632(get_type)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.eye}\n",
      "        5    0.000    0.000    0.000    0.000 storage.py:131(_normalize_store_arg_v2)\n",
      "        2    0.000    0.000    0.000    0.000 functional.py:484(crop)\n",
      "        2    0.000    0.000    0.000    0.000 numeric.py:1410(moveaxis)\n",
      "    18/14    0.000    0.000    0.000    0.000 base.py:189(_get_flag)\n",
      "       77    0.000    0.000    0.000    0.000 util.py:214(<genexpr>)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method torch.norm}\n",
      "        2    0.000    0.000    0.000    0.000 nodes.py:34(_set_value)\n",
      "       23    0.000    0.000    0.000    0.000 core.py:2131(_chunk_key)\n",
      "        2    0.000    0.000    0.000    0.000 omegaconf.py:888(_get_obj_type)\n",
      "       54    0.000    0.000    0.000    0.000 indexing.py:304(is_contiguous_slice)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.randperm}\n",
      "        5    0.000    0.000    0.000    0.000 indexing.py:229(replace_ellipsis)\n",
      "       16    0.000    0.000    0.000    0.000 _trace.py:1008(is_tracing)\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method torch.from_numpy}\n",
      "       92    0.000    0.000    0.000    0.000 indexing.py:159(ceildiv)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.arange}\n",
      "        4    0.000    0.000    0.000    0.000 functional.py:62(get_dimensions)\n",
      "        4    0.000    0.000    0.000    0.000 base.py:706(_maybe_resolve_interpolation)\n",
      "       10    0.000    0.000    0.000    0.000 _utils.py:440(is_attr_class)\n",
      "        2    0.000    0.000    0.000    0.000 nodes.py:56(validate_and_convert)\n",
      "        4    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)\n",
      "        1    0.000    0.000    0.000    0.000 rotation_conversions.py:580(matrix_to_rotation_6d)\n",
      "        5    0.000    0.000    0.000    0.000 decoder.py:343(raw_decode)\n",
      "       10    0.000    0.000    0.000    0.000 {method 'unsqueeze' of 'torch._C._TensorBase' objects}\n",
      "       10    0.000    0.000    0.000    0.000 _utils.py:432(is_dataclass)\n",
      "       10    0.000    0.000    0.000    0.000 store.py:92(_ensure_store)\n",
      "       77    0.000    0.000    0.000    0.000 core.py:483(<genexpr>)\n",
      "       38    0.000    0.000    0.000    0.000 functional_tensor.py:13(_assert_image_tensor)\n",
      "        2    0.000    0.000    0.000    0.000 functional_tensor.py:132(crop)\n",
      "        5    0.000    0.000    0.000    0.000 storage.py:842(__contains__)\n",
      "       32    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "      136    0.000    0.000    0.000    0.000 {method 'keys' of 'mappingproxy' objects}\n",
      "        2    0.000    0.000    0.000    0.000 _tensor.py:713(__iter__)\n",
      "        4    0.000    0.000    0.000    0.000 dictconfig.py:149(_validate_get)\n",
      "       16    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}\n",
      "     12/8    0.000    0.000    0.000    0.000 base.py:201(_get_flag_no_cache)\n",
      "        8    0.000    0.000    0.000    0.000 {method 'uniform_' of 'torch._C._TensorBase' objects}\n",
      "        2    0.000    0.000    0.000    0.000 <string>:2(__init__)\n",
      "       68    0.000    0.000    0.000    0.000 indexing.py:300(is_slice)\n",
      "        4    0.000    0.000    0.000    0.000 base.py:773(_invalidate_flags_cache)\n",
      "       32    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:398(parent)\n",
      "        4    0.000    0.000    0.000    0.000 omegaconf.py:956(read_write)\n",
      "        5    0.000    0.000    0.000    0.000 attrs.py:27(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'sum' of 'torch._C._TensorBase' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'clamp_min' of 'torch._C._TensorBase' objects}\n",
      "        8    0.000    0.000    0.000    0.000 basecontainer.py:694(_is_interpolation)\n",
      "       23    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n",
      "       14    0.000    0.000    0.000    0.000 indexing.py:18(is_integer)\n",
      "        4    0.000    0.000    0.000    0.000 numeric.py:1347(normalize_axis_tuple)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.cross}\n",
      "        5    0.000    0.000    0.000    0.000 meta.py:215(decode_fill_value)\n",
      "        2    0.000    0.000    0.000    0.000 registry.py:23(get_codec)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.cat}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method torch._C._get_tracing_state}\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
      "        2    0.000    0.000    0.000    0.000 _utils.py:596(_is_special)\n",
      "       15    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
      "        4    0.000    0.000    0.000    0.000 omegaconf.py:494(set_readonly)\n",
      "      137    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "      146    0.000    0.000    0.000    0.000 {built-in method math.ceil}\n",
      "        2    0.000    0.000    0.000    0.000 nodes.py:143(_validate_and_convert_impl)\n",
      "       55    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x5580ec479380}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method torch.empty}\n",
      "        5    0.000    0.000    0.000    0.000 meta.py:191(decode_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 basecontainer.py:560(get_target_type_hint)\n",
      "        4    0.000    0.000    0.000    0.000 dictconfig.py:144(_is_typed)\n",
      "        1    0.000    0.000    0.000    0.000 pytorch_util.py:6(dict_apply)\n",
      "        6    0.000    0.000    0.000    0.000 _VF.py:25(__getattr__)\n",
      "        4    0.000    0.000    0.000    0.000 _utils.py:620(is_primitive_list)\n",
      "       36    0.000    0.000    0.000    0.000 _utils.py:515(_is_missing_literal)\n",
      "       16    0.000    0.000    0.000    0.000 {built-in method torch._C._is_tracing}\n",
      "       38    0.000    0.000    0.000    0.000 functional_tensor.py:9(_is_tensor_a_torch_image)\n",
      "        4    0.000    0.000    0.000    0.000 dictconfig.py:274(_validate_and_normalize_key)\n",
      "       23    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
      "       28    0.000    0.000    0.000    0.000 indexing.py:274(ensure_tuple)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:88(__post_init__)\n",
      "       77    0.000    0.000    0.000    0.000 indexing.py:359(<genexpr>)\n",
      "        8    0.000    0.000    0.000    0.000 _utils.py:212(_resolve_optional)\n",
      "       19    0.000    0.000    0.000    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
      "       10    0.000    0.000    0.000    0.000 _funcs.py:326(has)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'clone' of 'torch._C._TensorBase' objects}\n",
      "        4    0.000    0.000    0.000    0.000 contextlib.py:261(helper)\n",
      "       77    0.000    0.000    0.000    0.000 indexing.py:361(<genexpr>)\n",
      "       48    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'view' of 'torch._C._TensorBase' objects}\n",
      "        4    0.000    0.000    0.000    0.000 nodes.py:113(_is_interpolation)\n",
      "       18    0.000    0.000    0.000    0.000 base.py:304(_is_none)\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method _codecs.decode}\n",
      "       10    0.000    0.000    0.000    0.000 dataclasses.py:1047(is_dataclass)\n",
      "       48    0.000    0.000    0.000    0.000 basecontainer.py:705(_value)\n",
      "        2    0.000    0.000    0.000    0.000 abc.py:97(from_config)\n",
      "       10    0.000    0.000    0.000    0.000 store.py:423(_prefix_to_array_key)\n",
      "        4    0.000    0.000    0.000    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'expand_as' of 'torch._C._TensorBase' objects}\n",
      "       77    0.000    0.000    0.000    0.000 indexing.py:360(<genexpr>)\n",
      "       36    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "        2    0.000    0.000    0.000    0.000 _utils.py:725(is_valid_value_annotation)\n",
      "        2    0.000    0.000    0.000    0.000 types.py:171(__get__)\n",
      "        2    0.000    0.000    0.000    0.000 omegaconf.py:947(<listcomp>)\n",
      "       55    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        4    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 process.py:234(ident)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'reshape' of 'torch._C._TensorBase' objects}\n",
      "        4    0.000    0.000    0.000    0.000 _utils.py:677(is_dict)\n",
      "        2    0.000    0.000    0.000    0.000 basecontainer.py:59(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 _utils.py:520(_is_none)\n",
      "       10    0.000    0.000    0.000    0.000 _utils.py:205(is_union_annotation)\n",
      "        6    0.000    0.000    0.000    0.000 _utils.py:626(is_primitive_dict)\n",
      "       40    0.000    0.000    0.000    0.000 util.py:345(<genexpr>)\n",
      "        9    0.000    0.000    0.000    0.000 core.py:367(shape)\n",
      "       14    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "       19    0.000    0.000    0.000    0.000 indexing.py:352(<genexpr>)\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method builtins.sum}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'squeeze' of 'torch._C._TensorBase' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'match' of 're.Pattern' objects}\n",
      "       14    0.000    0.000    0.000    0.000 {method 'indices' of 'slice' objects}\n",
      "        6    0.000    0.000    0.000    0.000 base.py:131(_get_parent)\n",
      "        4    0.000    0.000    0.000    0.000 dictconfig.py:277(_s_validate_and_normalize_key)\n",
      "       32    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
      "        6    0.000    0.000    0.000    0.000 base.py:181(_get_node_flag)\n",
      "        5    0.000    0.000    0.000    0.000 indexing.py:874(pop_fields)\n",
      "       28    0.000    0.000    0.000    0.000 core.py:359(chunk_store)\n",
      "       32    0.000    0.000    0.000    0.000 _jit_internal.py:958(is_scripting)\n",
      "        8    0.000    0.000    0.000    0.000 _utils.py:463(get_type_of)\n",
      "        4    0.000    0.000    0.000    0.000 numeric.py:1397(<listcomp>)\n",
      "       10    0.000    0.000    0.000    0.000 listconfig.py:84(_validate_get)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:364(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:1358(current_thread)\n",
      "       30    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
      "        4    0.000    0.000    0.000    0.000 base.py:65(<lambda>)\n",
      "        4    0.000    0.000    0.000    0.000 base.py:108(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 indexing.py:224(check_selection_length)\n",
      "       23    0.000    0.000    0.000    0.000 multiarray.py:1071(copyto)\n",
      "        5    0.000    0.000    0.000    0.000 store.py:447(_prefix_to_attrs_key)\n",
      "        2    0.000    0.000    0.000    0.000 _utils.py:631(is_dict_annotation)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.is_floating_point}\n",
      "        5    0.000    0.000    0.000    0.000 core.py:428(ndim)\n",
      "        5    0.000    0.000    0.000    0.000 indexing.py:61(is_pure_fancy_indexing)\n",
      "       10    0.000    0.000    0.000    0.000 _compat.py:90(get_generic_base)\n",
      "        2    0.000    0.000    0.000    0.000 functional_tensor.py:545(_cast_squeeze_in)\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function}\n",
      "        5    0.000    0.000    0.000    0.000 meta.py:183(_decode_dtype_descr)\n",
      "        2    0.000    0.000    0.000    0.000 enum.py:792(value)\n",
      "       10    0.000    0.000    0.000    0.000 indexing.py:839(check_fields)\n",
      "        5    0.000    0.000    0.000    0.000 indexing.py:234(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "       15    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
      "        5    0.000    0.000    0.000    0.000 indexing.py:657(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 store.py:312(_path_to_prefix)\n",
      "        6    0.000    0.000    0.000    0.000 {method 'is_floating_point' of 'torch._C._TensorBase' objects}\n",
      "        9    0.000    0.000    0.000    0.000 core.py:283(_refresh_metadata)\n",
      "       16    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "        6    0.000    0.000    0.000    0.000 base.py:128(_invalidate_flags_cache)\n",
      "       12    0.000    0.000    0.000    0.000 nodes.py:31(_value)\n",
      "        5    0.000    0.000    0.000    0.000 indexing.py:813(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 hierarchy.py:223(attrs)\n",
      "        6    0.000    0.000    0.000    0.000 base.py:344(_is_flags_root)\n",
      "        5    0.000    0.000    0.000    0.000 util.py:387(__init__)\n",
      "        9    0.000    0.000    0.000    0.000 core.py:386(dtype)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "        2    0.000    0.000    0.000    0.000 _utils.py:645(is_list_annotation)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'end' of 're.Match' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
      "        2    0.000    0.000    0.000    0.000 functional_tensor.py:561(_cast_squeeze_out)\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}\n",
      "        2    0.000    0.000    0.000    0.000 numeric.py:1472(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 _utils.py:655(is_tuple_annotation)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:1484(main_thread)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:67(__post_init__)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
      "        4    0.000    0.000    0.000    0.000 process.py:99(_check_closed)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_variadic}\n",
      "        4    0.000    0.000    0.000    0.000 process.py:37(current_process)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
      "        4    0.000    0.000    0.000    0.000 process.py:189(name)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method _operator.index}\n",
      "        2    0.000    0.000    0.000    0.000 numeric.py:1406(_moveaxis_dispatcher)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# profile dataset access\n",
    "import cProfile\n",
    "\n",
    "cProfile.run(\"dataset[0]\", sort=\"cumtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning, you should probably turn off EMA as it will cause performance issues with batchnorms. cannot replace batchnorms since you are using a pretrained model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterating dataset to get normalization:   3%|▎         | 14/498 [00:05<02:20,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterating dataset to get normalization:  19%|█▊        | 93/498 [00:05<00:12, 32.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterating dataset to get normalization:  32%|███▏      | 157/498 [00:06<00:04, 70.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterating dataset to get normalization:  44%|████▍     | 221/498 [00:06<00:02, 117.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterating dataset to get normalization:  56%|█████▌    | 280/498 [00:06<00:01, 163.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterating dataset to get normalization:  68%|██████▊   | 339/498 [00:06<00:00, 204.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterating dataset to get normalization:  74%|███████▍  | 371/498 [00:06<00:00, 221.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterating dataset to get normalization:  87%|████████▋ | 435/498 [00:07<00:00, 246.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterating dataset to get normalization:  94%|█████████▍| 467/498 [00:07<00:00, 255.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n",
      "dict_keys(['robot_eef_pose_6d_rot', 'gripper_width'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterating dataset to get normalization: 100%|██████████| 498/498 [00:07<00:00, 64.19it/s] \n"
     ]
    }
   ],
   "source": [
    "rgb = TimmRGBModel(model_name='resnet18',image_size=(224,224),pretrained=True)\n",
    "obs_encoder = MultiImageObsEncoder(shape_meta=shape_meta,rgb_model=rgb)\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=100)\n",
    "policy = DiffusionUnetImagePolicy(shape_meta=shape_meta,noise_scheduler=noise_scheduler,obs_encoder=obs_encoder,horizon=horizon,n_action_steps=n_action_steps,\n",
    "                                  n_obs_steps=n_obs_steps,num_inference_steps=100)\n",
    "policy.set_normalizer(dataset.get_normalizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2048383/2042096983.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs = {k: torch.tensor(v).unsqueeze(0).to(device) for k,v in obs.items()}\n"
     ]
    }
   ],
   "source": [
    "# benchmark inference time using pytorch profiler\n",
    "\n",
    "import torch.autograd.profiler as profiler\n",
    "import torch\n",
    "device=\"cuda:0\"\n",
    "policy.to(device)\n",
    "policy.num_inference_steps = 16 # DDIM inference iterations\n",
    "policy.eval()\n",
    "import time \n",
    "\n",
    "\n",
    "obs =  dataset[0][\"obs\"]\n",
    "obs = {k: torch.tensor(v).unsqueeze(0).to(device) for k,v in obs.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-02-14 23:26:34 2048383:2048383 init.cpp:111] function cbapi.getCuptiStatus() failed with error CUPTI_ERROR_NOT_INITIALIZED (15)\n",
      "WARNING:2025-02-14 23:26:34 2048383:2048383 init.cpp:112] CUPTI initialization failed - CUDA profiler activities will be missing\n",
      "INFO:2025-02-14 23:26:34 2048383:2048383 init.cpp:114] If you see CUPTI_ERROR_INSUFFICIENT_PRIVILEGES, refer to https://developer.nvidia.com/nvidia-development-tools-solutions-err-nvgpuctrperm-cupti\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "              aten::cudnn_convolution        93.01%        1.785s        93.03%        1.786s       3.144ms        1.832s        95.47%        1.833s       3.228ms           0 b           0 b      44.41 Mb    -231.77 Mb           568  \n",
      "                      model_inference         1.80%      34.549ms        99.97%        1.919s        1.919s      12.414ms         0.65%        1.919s        1.919s          -4 b      -1.42 Kb       1.00 Kb    -116.72 Mb             1  \n",
      "                        aten::nonzero         0.88%      16.982ms         0.90%      17.351ms     510.324us     920.000us         0.05%       1.258ms      37.000us           0 b           0 b           0 b           0 b            34  \n",
      "              aten::native_group_norm         0.69%      13.292ms         0.80%      15.400ms      38.500us       8.334ms         0.43%      12.645ms      31.613us           0 b           0 b       5.64 Mb      -1.78 Mb           400  \n",
      "                   aten::_convolution         0.57%      11.006ms        94.27%        1.810s       3.016ms       5.901ms         0.31%        1.852s       3.086ms           0 b           0 b      44.91 Mb     -32.00 Kb           600  \n",
      "                          aten::addmm         0.32%       6.111ms         0.33%       6.292ms      27.121us       6.870ms         0.36%       7.163ms      30.875us           0 b           0 b       1.75 Mb    -230.25 Mb           232  \n",
      "                            aten::mul         0.22%       4.158ms         0.22%       4.297ms      13.951us       3.959ms         0.21%       4.095ms      13.295us         128 b          64 b       4.86 Mb       4.86 Mb           308  \n",
      "                        aten::reshape         0.20%       3.896ms         0.24%       4.619ms       3.507us       3.321ms         0.17%       5.047ms       3.832us           0 b           0 b       2.48 Mb           0 b          1317  \n",
      "                      aten::unsqueeze         0.17%       3.233ms         0.18%       3.509ms       2.999us       2.982ms         0.16%       4.407ms       3.767us           0 b           0 b           0 b           0 b          1170  \n",
      "    aten::cudnn_convolution_transpose         0.14%       2.647ms         0.14%       2.658ms      83.062us       1.781ms         0.09%       1.819ms      56.844us           0 b           0 b     512.00 Kb     512.00 Kb            32  \n",
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.920s\n",
      "Self CUDA time total: 1.919s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def profile_inference():\n",
    "    with torch.no_grad():\n",
    "        with profiler.profile(record_shapes=True, profile_memory=True, use_cuda=True) as prof:\n",
    "            with profiler.record_function(\"model_inference\"):\n",
    "                for i in range(1):\n",
    "        \n",
    "                    action = policy.predict_action(obs)\n",
    "                    \n",
    "        print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))\n",
    "        \n",
    "profile_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2048383/1382257246.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"obs\"] = {k: torch.tensor(v).to(device) for k,v in batch[\"obs\"].items()}\n"
     ]
    }
   ],
   "source": [
    "# profile training \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = \"cuda:0\"\n",
    "policy.num_inference_steps = 100\n",
    "policy.train()\n",
    "policy.to(device)\n",
    "dataloader = DataLoader(dataset, batch_size=64, num_workers=8, shuffle=False)\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "batch[\"obs\"] = {k: torch.tensor(v).to(device) for k,v in batch[\"obs\"].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# with profiler.profile(record_shapes=False, profile_memory=False, use_cuda=False) as prof:\n",
    "#     with profiler.record_function(\"model_training\"):\n",
    "#         for i in range(2):\n",
    "#             # mark in the profiler the forward pass\n",
    "#             batch = next(iter(dataloader))\n",
    "#             loss = policy.compute_loss(batch)\n",
    "#             loss.backward()\n",
    "\n",
    "# print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "# # dump trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/157 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/157 [00:05<14:12,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 5.302886486053467, avg transfer time: 0.07214236259460449, avg forward pass time: 0.05287671089172363, avg backward pass time: 0.03538036346435547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 11/157 [00:09<01:16,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.6511701887304132, avg transfer time: 0.11558123068376021, avg forward pass time: 0.05069884386929599, avg backward pass time: 0.03468829935247248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 21/157 [00:13<00:48,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.4319522040230887, avg transfer time: 0.11910705339340937, avg forward pass time: 0.0506657532283238, avg backward pass time: 0.034343231291998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 31/157 [00:17<00:34,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.34965273641770883, avg transfer time: 0.121599058951101, avg forward pass time: 0.05057584085772114, avg backward pass time: 0.032881613700620586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 41/157 [00:22<01:27,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.3528158373949004, avg transfer time: 0.12048303208700041, avg forward pass time: 0.050628534177454505, avg backward pass time: 0.032601001786022654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 51/157 [00:26<00:51,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.3188122908274333, avg transfer time: 0.12093687057495117, avg forward pass time: 0.05046578949572993, avg backward pass time: 0.03317963375764735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 61/157 [00:30<00:32,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.29629582264384285, avg transfer time: 0.12086036947906995, avg forward pass time: 0.050188209189743295, avg backward pass time: 0.03334282265334833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 71/157 [00:34<00:23,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.2785705008976896, avg transfer time: 0.12108382708589796, avg forward pass time: 0.050056870554534484, avg backward pass time: 0.03355479912019112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 81/157 [00:39<00:55,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.286959241937708, avg transfer time: 0.11995075955803011, avg forward pass time: 0.04990117343855493, avg backward pass time: 0.03377725459911205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 92/157 [00:43<00:24,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.27447179909590835, avg transfer time: 0.12013120703644804, avg forward pass time: 0.049764475979647796, avg backward pass time: 0.03300677813016451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 101/157 [00:47<00:18,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.2649486985537085, avg transfer time: 0.1207282566788173, avg forward pass time: 0.049646058885177764, avg backward pass time: 0.03170520244258465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 111/157 [00:50<00:12,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.2571296455623867, avg transfer time: 0.12134546632165308, avg forward pass time: 0.049596707026163735, avg backward pass time: 0.03105159278388496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 122/157 [00:56<00:20,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.2645910219712691, avg transfer time: 0.12087738218386311, avg forward pass time: 0.04950103877989714, avg backward pass time: 0.030647433493748183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 132/157 [01:00<00:09,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.25755808189625046, avg transfer time: 0.12111719692026386, avg forward pass time: 0.04943369181101559, avg backward pass time: 0.030248978665766826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 141/157 [01:03<00:05,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.25156979696125004, avg transfer time: 0.12125991253142661, avg forward pass time: 0.049379998065055684, avg backward pass time: 0.02991788776208323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 152/157 [01:06<00:01,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.24210178931027848, avg transfer time: 0.12046184129272865, avg forward pass time: 0.049125188233836596, avg backward pass time: 0.02938002466365991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:08<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg load time: 0.23542438494931361, avg transfer time: 0.11922295382068415, avg forward pass time: 0.04869802438529434, avg backward pass time: 0.028978886877655225\n",
      "n_workers: 8, time: 68.28810906410217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataloaders with different numbers of workers, and compare the performance for loading the entire dataset \n",
    "import time\n",
    "batch_size = 64\n",
    "# create smaller dataset split\n",
    "split,_= torch.utils.data.random_split(dataset, [10000, len(dataset)-10000])\n",
    "print(len(split)//batch_size)\n",
    "import tqdm\n",
    "\n",
    "def do_one_pass(dataloader):\n",
    "    avg_time = 0\n",
    "    start = time.time()\n",
    "    load_times = []\n",
    "    transfer_times = []\n",
    "    forward_pass_times = []\n",
    "    backward_pass_times = []\n",
    "\n",
    "    stop_time = time.time()\n",
    "    for i, batch in enumerate(tqdm.tqdm(dataloader)):\n",
    "        loaded_time = time.time()\n",
    "        for k in batch[\"obs\"]:\n",
    "            batch[\"obs\"][k] = batch[\"obs\"][k].to(device)\n",
    "        batch[\"action\"] = batch[\"action\"].to(device)\n",
    "        transferred_time = time.time()\n",
    "        loss = policy.compute_loss(batch)\n",
    "        forwarded_time = time.time()\n",
    "        loss.backward()\n",
    "        backwared_time = time.time()\n",
    "\n",
    "        load_times.append(loaded_time-stop_time)\n",
    "        transfer_times.append(transferred_time-loaded_time)\n",
    "        forward_pass_times.append(forwarded_time-transferred_time)\n",
    "        backward_pass_times.append(backwared_time-forwarded_time)\n",
    "        stop_time = time.time()\n",
    "\n",
    "        # add average times to the tqdm description\n",
    "        if i%10 == 0:\n",
    "            tqdm.tqdm.write(f\"avg load time: {sum(load_times)/len(load_times)}, avg transfer time: {sum(transfer_times)/len(transfer_times)}, avg forward pass time: {sum(forward_pass_times)/len(forward_pass_times)}, avg backward pass time: {sum(backward_pass_times)/len(backward_pass_times)}\")\n",
    "\n",
    "    print(f\"avg load time: {sum(load_times)/len(load_times)}, avg transfer time: {sum(transfer_times)/len(transfer_times)}, avg forward pass time: {sum(forward_pass_times)/len(forward_pass_times)}, avg backward pass time: {sum(backward_pass_times)/len(backward_pass_times)}\")\n",
    "\n",
    "    return time.time()-start\n",
    "\n",
    "for n_workers in [8]:\n",
    "    dataloader = DataLoader(split, batch_size=batch_size, num_workers=n_workers, shuffle=True, pin_memory=False,persistent_workers=True)\n",
    "    print(f\"n_workers: {n_workers}, time: {do_one_pass(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004042625427246094\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "d = {k: torch.rand(2,3,224,224).to(device) for k in  range(4)}\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         10717423 function calls (10707488 primitive calls) in 17.746 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       11    0.000    0.000   17.751    1.614 dataloader.py:676(__next__)\n",
      "       11    0.002    0.000   17.750    1.614 dataloader.py:719(_next_data)\n",
      "       11    0.000    0.000   17.748    1.613 fetch.py:47(fetch)\n",
      "       11    0.005    0.000   17.419    1.584 fetch.py:49(<listcomp>)\n",
      "      704    0.115    0.000   17.414    0.025 real_image_dataset.py:286(__getitem__)\n",
      "     1408    0.009    0.000   14.020    0.010 transforms.py:92(__call__)\n",
      "     4224    0.013    0.000   14.011    0.003 module.py:1124(_call_impl)\n",
      "     1408    0.059    0.000   12.465    0.009 transforms.py:1225(forward)\n",
      "     1408    0.007    0.000   10.999    0.008 functional.py:866(adjust_hue)\n",
      "     1408    0.172    0.000   10.989    0.008 functional_tensor.py:193(adjust_hue)\n",
      "     1408    0.672    0.000    8.703    0.006 functional_tensor.py:301(_hsv2rgb)\n",
      "     1408    0.012    0.000    6.610    0.005 functional.py:192(einsum)\n",
      "     1408    6.590    0.005    6.590    0.005 {built-in method torch.einsum}\n",
      "      704    0.005    0.000    2.628    0.004 real_image_dataset.py:278(_get_from_sampler)\n",
      "      704    0.063    0.000    2.624    0.004 sampler.py:125(sample_sequence)\n",
      "     1408    1.129    0.001    2.058    0.001 functional_tensor.py:262(_rgb2hsv)\n",
      "     3520    0.005    0.000    1.978    0.001 core.py:648(__getitem__)\n",
      "     3520    0.005    0.000    1.968    0.001 core.py:791(get_basic_selection)\n",
      "     3520    0.005    0.000    1.962    0.001 core.py:951(_get_basic_selection_nd)\n",
      "     3520    0.019    0.000    1.894    0.001 core.py:1219(_get_selection)\n",
      "    16721    0.031    0.000    1.766    0.000 core.py:1906(_chunk_getitem)\n",
      "     1408    0.003    0.000    1.467    0.001 transforms.py:341(forward)\n",
      "     1408    0.008    0.000    1.465    0.001 functional.py:363(resize)\n",
      "     1408    0.011    0.000    1.445    0.001 functional_tensor.py:429(resize)\n",
      "     1408    0.008    0.000    1.428    0.001 functional.py:3757(interpolate)\n",
      "     1408    1.419    0.001    1.419    0.001 {built-in method torch._C._nn.upsample_bilinear2d}\n",
      "     9911    1.383    0.000    1.383    0.000 {built-in method torch.stack}\n",
      "    16721    0.506    0.000    1.211    0.000 core.py:1823(_process_chunk)\n",
      "   760328    0.074    0.000    1.195    0.000 {built-in method builtins.isinstance}\n",
      "    48930    0.023    0.000    1.133    0.000 compat.py:13(ensure_ndarray_like)\n",
      "    48930    0.008    0.000    1.097    0.000 ndarray_like.py:54(is_ndarray_like)\n",
      "    48930    0.045    0.000    1.072    0.000 typing.py:1141(__instancecheck__)\n",
      "    39778    0.014    0.000    0.925    0.000 compat.py:48(ensure_ndarray)\n",
      "     4224    0.714    0.000    0.826    0.000 functional_tensor.py:256(_blend)\n",
      "    97860    0.499    0.000    0.705    0.000 typing.py:1065(_get_protocol_attrs)\n",
      "     1408    0.004    0.000    0.682    0.000 functional.py:844(adjust_saturation)\n",
      "     1408    0.004    0.000    0.671    0.000 functional_tensor.py:222(adjust_saturation)\n",
      "     9152    0.005    0.000    0.486    0.000 compat.py:126(ensure_contiguous_ndarray)\n",
      "     3520    0.004    0.000    0.457    0.000 replay_buffer.py:415(__getitem__)\n",
      "     3520    0.014    0.000    0.453    0.000 hierarchy.py:388(__getitem__)\n",
      "    48930    0.030    0.000    0.440    0.000 typing.py:1082(_is_callable_members_only)\n",
      "     2816    0.377    0.000    0.397    0.000 functional_tensor.py:145(rgb_to_grayscale)\n",
      "     1408    0.004    0.000    0.391    0.000 functional.py:822(adjust_contrast)\n",
      "   131302    0.047    0.000    0.389    0.000 {built-in method builtins.all}\n",
      "     3520    0.018    0.000    0.383    0.000 core.py:154(__init__)\n",
      "     1408    0.007    0.000    0.380    0.000 functional_tensor.py:176(adjust_contrast)\n",
      "    77/11    0.000    0.000    0.329    0.030 collate.py:84(default_collate)\n",
      "    22/11    0.000    0.000    0.328    0.030 collate.py:160(<dictcomp>)\n",
      "    11264    0.326    0.000    0.326    0.000 {method 'to' of 'torch._C._TensorBase' objects}\n",
      "     3520    0.002    0.000    0.320    0.000 core.py:217(_load_metadata)\n",
      "     3520    0.012    0.000    0.318    0.000 core.py:226(_load_metadata_nosync)\n",
      "     1408    0.305    0.000    0.305    0.000 {built-in method torch.max}\n",
      "     1408    0.296    0.000    0.296    0.000 {built-in method torch.min}\n",
      "     3520    0.012    0.000    0.291    0.000 meta.py:108(decode_array_metadata)\n",
      "     1408    0.004    0.000    0.286    0.000 functional.py:800(adjust_brightness)\n",
      "     1408    0.004    0.000    0.275    0.000 functional_tensor.py:165(adjust_brightness)\n",
      "     3520    0.002    0.000    0.263    0.000 meta.py:90(parse_metadata)\n",
      "     1408    0.003    0.000    0.258    0.000 listconfig.py:512(__eq__)\n",
      "     3520    0.005    0.000    0.258    0.000 util.py:54(json_loads)\n",
      "     9152    0.015    0.000    0.249    0.000 compat.py:70(ensure_contiguous_ndarray_like)\n",
      "     3520    0.004    0.000    0.220    0.000 compat.py:178(ensure_text)\n",
      "   470817    0.115    0.000    0.220    0.000 typing.py:1149(<genexpr>)\n",
      "     1408    0.006    0.000    0.204    0.000 listconfig.py:46(__init__)\n",
      "     1408    0.001    0.000    0.185    0.000 listconfig.py:610(_set_value)\n",
      "     1408    0.007    0.000    0.183    0.000 listconfig.py:620(_set_value_impl)\n",
      "     1408    0.017    0.000    0.140    0.000 real_image_dataset.py:35(position_and_rot6d_to_se3)\n",
      "     1408    0.002    0.000    0.138    0.000 listconfig.py:293(append)\n",
      "     1408    0.007    0.000    0.136    0.000 basecontainer.py:522(_set_item_impl)\n",
      "  2559144    0.135    0.000    0.135    0.000 {method 'startswith' of 'str' objects}\n",
      "    18129    0.118    0.000    0.132    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "     2816    0.127    0.000    0.127    0.000 {built-in method torch.where}\n",
      "    16721    0.008    0.000    0.126    0.000 <__array_function__ internals>:177(copyto)\n",
      "     1408    0.119    0.000    0.119    0.000 {built-in method torch.fmod}\n",
      "     2816    0.003    0.000    0.116    0.000 numeric.py:289(full)\n",
      "     4224    0.109    0.000    0.109    0.000 {method 'clamp' of 'torch._C._TensorBase' objects}\n",
      "    20241    0.039    0.000    0.105    0.000 indexing.py:356(__iter__)\n",
      "    16721    0.007    0.000    0.100    0.000 indexing.py:312(is_contiguous_selection)\n",
      "     1408    0.019    0.000    0.093    0.000 rotation_conversions.py:556(rotation_6d_to_matrix)\n",
      "    55795    0.014    0.000    0.083    0.000 indexing.py:314(<genexpr>)\n",
      "     3520    0.078    0.000    0.078    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "     5632    0.004    0.000    0.076    0.000 _tensor.py:26(wrapped)\n",
      "   812444    0.076    0.000    0.076    0.000 {built-in method builtins.getattr}\n",
      "     2816    0.002    0.000    0.075    0.000 dictconfig.py:361(__getitem__)\n",
      "     2816    0.004    0.000    0.073    0.000 dictconfig.py:438(_get_impl)\n",
      "     5632    0.004    0.000    0.072    0.000 _tensor.py:637(__rsub__)\n",
      "     5632    0.068    0.000    0.068    0.000 {built-in method torch.rsub}\n",
      "     3520    0.016    0.000    0.064    0.000 indexing.py:328(__init__)\n",
      "     1408    0.003    0.000    0.062    0.000 basecontainer.py:620(_wrap_value_and_set)\n",
      "     1408    0.005    0.000    0.060    0.000 transforms.py:655(forward)\n",
      "     1408    0.001    0.000    0.058    0.000 omegaconf.py:1076(_maybe_wrap)\n",
      "     1408    0.004    0.000    0.057    0.000 omegaconf.py:979(_node_wrap)\n",
      "   186059    0.022    0.000    0.055    0.000 abc.py:117(__instancecheck__)\n",
      "    39074    0.014    0.000    0.053    0.000 indexing.py:36(is_integer_array)\n",
      "  1370040    0.052    0.000    0.052    0.000 {method 'add' of 'set' objects}\n",
      "     1408    0.003    0.000    0.052    0.000 listconfig.py:662(_list_eq)\n",
      "     2816    0.012    0.000    0.050    0.000 functional.py:4598(normalize)\n",
      "    20241    0.028    0.000    0.049    0.000 storage.py:816(__getitem__)\n",
      "     1408    0.006    0.000    0.047    0.000 listconfig.py:90(_validate_set)\n",
      "   522209    0.047    0.000    0.047    0.000 {built-in method builtins.hasattr}\n",
      "    32913    0.030    0.000    0.047    0.000 indexing.py:178(__iter__)\n",
      "     2816    0.003    0.000    0.044    0.000 basecontainer.py:86(_resolve_with_default)\n",
      "     4224    0.042    0.000    0.042    0.000 {built-in method torch.clamp}\n",
      "     1408    0.003    0.000    0.040    0.000 nodes.py:128(__init__)\n",
      "     1408    0.010    0.000    0.040    0.000 transforms.py:1193(get_params)\n",
      "    22528    0.017    0.000    0.039    0.000 _utils.py:507(_is_missing_value)\n",
      "    10560    0.025    0.000    0.038    0.000 util.py:309(normalize_storage_path)\n",
      "    39074    0.017    0.000    0.037    0.000 numeric.py:1873(isscalar)\n",
      "     1408    0.037    0.000    0.037    0.000 {built-in method torch.zeros_like}\n",
      "     5632    0.004    0.000    0.037    0.000 basecontainer.py:64(_get_child)\n",
      "    16721    0.011    0.000    0.035    0.000 util.py:200(is_total_slice)\n",
      "     1408    0.004    0.000    0.035    0.000 nodes.py:24(__init__)\n",
      "     1408    0.034    0.000    0.034    0.000 {built-in method torch.mean}\n",
      "   186059    0.033    0.000    0.033    0.000 {built-in method _abc._abc_instancecheck}\n",
      "     3520    0.004    0.000    0.033    0.000 __init__.py:299(loads)\n",
      "     1408    0.004    0.000    0.031    0.000 basecontainer.py:637(_item_eq)\n",
      "     3520    0.006    0.000    0.031    0.000 storage.py:101(contains_array)\n",
      "     5643    0.002    0.000    0.031    0.000 {built-in method builtins.next}\n",
      "     1408    0.005    0.000    0.031    0.000 transforms.py:620(get_params)\n",
      "    97860    0.015    0.000    0.029    0.000 typing.py:1084(<genexpr>)\n",
      "     3520    0.005    0.000    0.028    0.000 decoder.py:332(decode)\n",
      "     7040    0.003    0.000    0.027    0.000 _utils.py:448(is_structured_config)\n",
      "     2816    0.003    0.000    0.026    0.000 _tensor.py:527(norm)\n",
      "    11264    0.014    0.000    0.026    0.000 utils.py:543(_log_api_usage_once)\n",
      "134118/129892    0.009    0.000    0.026    0.000 {built-in method builtins.len}\n",
      "     5632    0.012    0.000    0.026    0.000 _utils.py:747(_get_value)\n",
      "    16721    0.013    0.000    0.026    0.000 core.py:478(_cdata_shape)\n",
      "     5632    0.005    0.000    0.026    0.000 basecontainer.py:179(__len__)\n",
      "     8448    0.025    0.000    0.025    0.000 {method 'unbind' of 'torch._C._TensorBase' objects}\n",
      "    23761    0.019    0.000    0.025    0.000 storage.py:789(_get_parent)\n",
      "     3520    0.006    0.000    0.025    0.000 hierarchy.py:360(_item_path)\n",
      "      704    0.023    0.000    0.023    0.000 {built-in method torch.matmul}\n",
      "     2816    0.007    0.000    0.023    0.000 functional.py:1345(norm)\n",
      "     7040    0.004    0.000    0.022    0.000 _utils.py:540(get_value_kind)\n",
      "     9856    0.011    0.000    0.022    0.000 indexing.py:165(__init__)\n",
      "    18304    0.018    0.000    0.022    0.000 functional_tensor.py:24(get_dimensions)\n",
      "    12672    0.002    0.000    0.022    0.000 base.py:298(_is_missing)\n",
      "     3520    0.002    0.000    0.022    0.000 storage.py:158(normalize_store_arg)\n",
      "      704    0.022    0.000    0.022    0.000 {built-in method torch._C._linalg.linalg_inv}\n",
      "    63539    0.022    0.000    0.022    0.000 {built-in method numpy.array}\n",
      "    18304    0.008    0.000    0.022    0.000 _utils.py:566(_is_interpolation)\n",
      "     2816    0.001    0.000    0.021    0.000 contextlib.py:114(__enter__)\n",
      "     2816    0.003    0.000    0.020    0.000 omegaconf.py:936(flag_override)\n",
      "     1408    0.020    0.000    0.020    0.000 {built-in method torch.floor}\n",
      "     3520    0.005    0.000    0.020    0.000 storage.py:131(_normalize_store_arg_v2)\n",
      "     5632    0.009    0.000    0.020    0.000 base.py:152(_set_flag)\n",
      "     2816    0.003    0.000    0.020    0.000 dictconfig.py:455(_get_node)\n",
      "     5632    0.006    0.000    0.019    0.000 listconfig.py:402(_get_node)\n",
      "     1408    0.019    0.000    0.019    0.000 {built-in method torch.ones_like}\n",
      "     2816    0.018    0.000    0.018    0.000 {built-in method torch.randint}\n",
      "   450576    0.018    0.000    0.018    0.000 {built-in method builtins.callable}\n",
      "     1408    0.017    0.000    0.017    0.000 {method 'repeat' of 'torch._C._TensorBase' objects}\n",
      "12672/9856    0.008    0.000    0.017    0.000 base.py:189(_get_flag)\n",
      "     1408    0.002    0.000    0.017    0.000 nodes.py:34(_set_value)\n",
      "    47872    0.012    0.000    0.017    0.000 <frozen importlib._bootstrap>:1033(_handle_fromlist)\n",
      "    55795    0.015    0.000    0.017    0.000 util.py:214(<genexpr>)\n",
      "    16721    0.008    0.000    0.017    0.000 core.py:2131(_chunk_key)\n",
      "     1408    0.001    0.000    0.016    0.000 <__array_function__ internals>:177(moveaxis)\n",
      "      704    0.005    0.000    0.016    0.000 real_image_dataset.py:51(se3_to_position_and_rot6d)\n",
      "     1408    0.001    0.000    0.016    0.000 omegaconf.py:632(get_type)\n",
      "     8448    0.004    0.000    0.016    0.000 functional_tensor.py:62(_assert_channels)\n",
      "     1408    0.003    0.000    0.015    0.000 functional.py:484(crop)\n",
      "     3520    0.015    0.000    0.015    0.000 decoder.py:343(raw_decode)\n",
      "    39074    0.009    0.000    0.015    0.000 indexing.py:304(is_contiguous_slice)\n",
      "     1408    0.003    0.000    0.015    0.000 omegaconf.py:888(_get_obj_type)\n",
      "    65826    0.010    0.000    0.015    0.000 indexing.py:159(ceildiv)\n",
      "     1408    0.014    0.000    0.014    0.000 {built-in method torch.randperm}\n",
      "     7040    0.010    0.000    0.014    0.000 store.py:92(_ensure_store)\n",
      "     1408    0.005    0.000    0.014    0.000 numeric.py:1410(moveaxis)\n",
      "     1408    0.001    0.000    0.013    0.000 nodes.py:56(validate_and_convert)\n",
      "     2816    0.004    0.000    0.013    0.000 functional.py:62(get_dimensions)\n",
      "     1408    0.013    0.000    0.013    0.000 {built-in method torch.arange}\n",
      "     2816    0.013    0.000    0.013    0.000 {built-in method torch.norm}\n",
      "    55795    0.010    0.000    0.013    0.000 core.py:483(<genexpr>)\n",
      "     7040    0.006    0.000    0.013    0.000 _utils.py:440(is_attr_class)\n",
      "     2816    0.002    0.000    0.013    0.000 base.py:706(_maybe_resolve_interpolation)\n",
      "     3520    0.007    0.000    0.013    0.000 indexing.py:229(replace_ellipsis)\n",
      "     2816    0.002    0.000    0.013    0.000 contextlib.py:123(__exit__)\n",
      "     2816    0.003    0.000    0.012    0.000 dictconfig.py:149(_validate_get)\n",
      "     3520    0.006    0.000    0.012    0.000 storage.py:842(__contains__)\n",
      "     7040    0.005    0.000    0.011    0.000 _utils.py:432(is_dataclass)\n",
      "    23057    0.011    0.000    0.011    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "    97860    0.011    0.000    0.011    0.000 {method 'keys' of 'mappingproxy' objects}\n",
      "     7040    0.011    0.000    0.011    0.000 {method 'unsqueeze' of 'torch._C._TensorBase' objects}\n",
      "    11264    0.007    0.000    0.011    0.000 _trace.py:1008(is_tracing)\n",
      "      704    0.003    0.000    0.011    0.000 rotation_conversions.py:580(matrix_to_rotation_6d)\n",
      "     1408    0.011    0.000    0.011    0.000 {built-in method torch.eye}\n",
      "    26752    0.005    0.000    0.010    0.000 functional_tensor.py:13(_assert_image_tensor)\n",
      "     1408    0.008    0.000    0.010    0.000 functional_tensor.py:132(crop)\n",
      "     3520    0.004    0.000    0.009    0.000 attrs.py:27(__init__)\n",
      "     3520    0.004    0.000    0.009    0.000 meta.py:215(decode_fill_value)\n",
      "    11264    0.009    0.000    0.009    0.000 {built-in method torch._C._log_api_usage_once}\n",
      "     2816    0.002    0.000    0.009    0.000 omegaconf.py:956(read_write)\n",
      "     1408    0.003    0.000    0.009    0.000 <string>:2(__init__)\n",
      "     2816    0.006    0.000    0.009    0.000 base.py:773(_invalidate_flags_cache)\n",
      "     5632    0.009    0.000    0.009    0.000 {method 'uniform_' of 'torch._C._TensorBase' objects}\n",
      "8448/5632    0.004    0.000    0.009    0.000 base.py:201(_get_flag_no_cache)\n",
      "     1408    0.002    0.000    0.009    0.000 _tensor.py:713(__iter__)\n",
      "     1408    0.003    0.000    0.009    0.000 registry.py:23(get_codec)\n",
      "    16721    0.008    0.000    0.008    0.000 {method 'join' of 'str' objects}\n",
      "    22528    0.006    0.000    0.008    0.000 <frozen importlib._bootstrap>:398(parent)\n",
      "    48930    0.006    0.000    0.008    0.000 indexing.py:300(is_slice)\n",
      "     5632    0.002    0.000    0.008    0.000 basecontainer.py:694(_is_interpolation)\n",
      "    10560    0.004    0.000    0.008    0.000 {built-in method builtins.any}\n",
      "     9856    0.003    0.000    0.008    0.000 indexing.py:18(is_integer)\n",
      "     1408    0.008    0.000    0.008    0.000 {built-in method torch.cross}\n",
      "     2816    0.007    0.000    0.007    0.000 {method 'clamp_min' of 'torch._C._TensorBase' objects}\n",
      "   104900    0.007    0.000    0.007    0.000 {built-in method math.ceil}\n",
      "     1408    0.007    0.000    0.007    0.000 {built-in method torch.cat}\n",
      "     1408    0.007    0.000    0.007    0.000 {method 'sum' of 'torch._C._TensorBase' objects}\n",
      "      245    0.007    0.000    0.007    0.000 {built-in method numpy.zeros}\n",
      "     3520    0.007    0.000    0.007    0.000 {built-in method torch.from_numpy}\n",
      "     7040    0.007    0.000    0.007    0.000 {method 'match' of 're.Pattern' objects}\n",
      "     5632    0.006    0.000    0.006    0.000 {built-in method torch._C._get_tracing_state}\n",
      "     2816    0.004    0.000    0.006    0.000 numeric.py:1347(normalize_axis_tuple)\n",
      "     3520    0.005    0.000    0.006    0.000 meta.py:191(decode_dtype)\n",
      "     1408    0.002    0.000    0.006    0.000 nodes.py:143(_validate_and_convert_impl)\n",
      "     1408    0.001    0.000    0.006    0.000 _utils.py:596(_is_special)\n",
      "     5633    0.006    0.000    0.006    0.000 {built-in method torch.empty}\n",
      "     2816    0.001    0.000    0.006    0.000 omegaconf.py:494(set_readonly)\n",
      "    98564    0.006    0.000    0.006    0.000 {method 'keys' of 'dict' objects}\n",
      "     6336    0.006    0.000    0.006    0.000 {built-in method numpy.empty}\n",
      "     2816    0.002    0.000    0.006    0.000 dictconfig.py:144(_is_typed)\n",
      "     2816    0.002    0.000    0.006    0.000 contextlib.py:261(helper)\n",
      "    55795    0.005    0.000    0.005    0.000 indexing.py:359(<genexpr>)\n",
      "    16721    0.003    0.000    0.005    0.000 <string>:1(<lambda>)\n",
      "     4224    0.003    0.000    0.005    0.000 _VF.py:25(__getattr__)\n",
      "     3520    0.005    0.000    0.005    0.000 {built-in method _codecs.decode}\n",
      "    34321    0.005    0.000    0.005    0.000 {method 'split' of 'str' objects}\n",
      "     1408    0.001    0.000    0.005    0.000 basecontainer.py:560(get_target_type_hint)\n",
      "     1408    0.005    0.000    0.005    0.000 abc.py:97(from_config)\n",
      "     2816    0.003    0.000    0.005    0.000 process.py:234(ident)\n",
      "    26752    0.005    0.000    0.005    0.000 functional_tensor.py:9(_is_tensor_a_torch_image)\n",
      "    55795    0.005    0.000    0.005    0.000 indexing.py:361(<genexpr>)\n",
      "    25344    0.003    0.000    0.005    0.000 _utils.py:515(_is_missing_literal)\n",
      "    13905    0.005    0.000    0.005    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
      "     2816    0.003    0.000    0.005    0.000 _utils.py:620(is_primitive_list)\n",
      "     1408    0.001    0.000    0.005    0.000 base.py:88(__post_init__)\n",
      "    20241    0.004    0.000    0.005    0.000 indexing.py:274(ensure_tuple)\n",
      "    39778    0.005    0.000    0.005    0.000 {built-in method __new__ of type object at 0x5580ec479380}\n",
      "      704    0.002    0.000    0.004    0.000 pytorch_util.py:6(dict_apply)\n",
      "     7040    0.003    0.000    0.004    0.000 _funcs.py:326(has)\n",
      "     2816    0.001    0.000    0.004    0.000 nodes.py:113(_is_interpolation)\n",
      "     5632    0.002    0.000    0.004    0.000 _utils.py:212(_resolve_optional)\n",
      "    12672    0.003    0.000    0.004    0.000 base.py:304(_is_none)\n",
      "     2816    0.002    0.000    0.004    0.000 dictconfig.py:274(_validate_and_normalize_key)\n",
      "     2816    0.004    0.000    0.004    0.000 contextlib.py:86(__init__)\n",
      "    55795    0.004    0.000    0.004    0.000 indexing.py:360(<genexpr>)\n",
      "     7040    0.002    0.000    0.004    0.000 dataclasses.py:1047(is_dataclass)\n",
      "     2816    0.004    0.000    0.004    0.000 {method 'expand_as' of 'torch._C._TensorBase' objects}\n",
      "    38720    0.004    0.000    0.004    0.000 {method 'get' of 'dict' objects}\n",
      "    25873    0.004    0.000    0.004    0.000 {built-in method builtins.min}\n",
      "    33792    0.004    0.000    0.004    0.000 basecontainer.py:705(_value)\n",
      "     1408    0.004    0.000    0.004    0.000 {method 'view' of 'torch._C._TensorBase' objects}\n",
      "    11264    0.004    0.000    0.004    0.000 {built-in method torch._C._is_tracing}\n",
      "     2817    0.004    0.000    0.004    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
      "     2816    0.001    0.000    0.004    0.000 _utils.py:677(is_dict)\n",
      "      704    0.004    0.000    0.004    0.000 {method 'clone' of 'torch._C._TensorBase' objects}\n",
      "    28160    0.004    0.000    0.004    0.000 util.py:345(<genexpr>)\n",
      "     1408    0.002    0.000    0.003    0.000 types.py:171(__get__)\n",
      "     7040    0.002    0.000    0.003    0.000 store.py:423(_prefix_to_array_key)\n",
      "      704    0.003    0.000    0.003    0.000 {method 'reshape' of 'torch._C._TensorBase' objects}\n",
      "     1408    0.001    0.000    0.003    0.000 basecontainer.py:59(__init__)\n",
      "     2816    0.001    0.000    0.003    0.000 _utils.py:520(_is_none)\n",
      "     4224    0.002    0.000    0.003    0.000 _utils.py:626(is_primitive_dict)\n",
      "     1408    0.001    0.000    0.003    0.000 _utils.py:725(is_valid_value_annotation)\n",
      "      704    0.003    0.000    0.003    0.000 {method 'squeeze' of 'torch._C._TensorBase' objects}\n",
      "     1408    0.001    0.000    0.003    0.000 omegaconf.py:947(<listcomp>)\n",
      "     7040    0.002    0.000    0.003    0.000 _utils.py:205(is_union_annotation)\n",
      "     9856    0.003    0.000    0.003    0.000 {method 'indices' of 'slice' objects}\n",
      "    13376    0.002    0.000    0.003    0.000 indexing.py:352(<genexpr>)\n",
      "    20241    0.003    0.000    0.003    0.000 core.py:359(chunk_store)\n",
      "     4224    0.002    0.000    0.003    0.000 base.py:181(_get_node_flag)\n",
      "     3520    0.002    0.000    0.002    0.000 {built-in method builtins.sum}\n",
      "     4224    0.001    0.000    0.002    0.000 base.py:131(_get_parent)\n",
      "    22528    0.002    0.000    0.002    0.000 {method 'rpartition' of 'str' objects}\n",
      "     9856    0.002    0.000    0.002    0.000 {built-in method builtins.max}\n",
      "     3520    0.002    0.000    0.002    0.000 indexing.py:874(pop_fields)\n",
      "     2816    0.002    0.000    0.002    0.000 threading.py:1358(current_thread)\n",
      "     5632    0.002    0.000    0.002    0.000 _utils.py:463(get_type_of)\n",
      "     2816    0.002    0.000    0.002    0.000 dictconfig.py:277(_s_validate_and_normalize_key)\n",
      "     1408    0.001    0.000    0.002    0.000 base.py:364(__init__)\n",
      "    21120    0.002    0.000    0.002    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
      "     7040    0.001    0.000    0.002    0.000 listconfig.py:84(_validate_get)\n",
      "    22528    0.002    0.000    0.002    0.000 _jit_internal.py:958(is_scripting)\n",
      "     3061    0.001    0.000    0.002    0.000 core.py:367(shape)\n",
      "     1408    0.001    0.000    0.002    0.000 _utils.py:631(is_dict_annotation)\n",
      "    16721    0.002    0.000    0.002    0.000 multiarray.py:1071(copyto)\n",
      "     2816    0.002    0.000    0.002    0.000 base.py:65(<lambda>)\n",
      "     3520    0.001    0.000    0.002    0.000 store.py:447(_prefix_to_attrs_key)\n",
      "     2816    0.001    0.000    0.002    0.000 numeric.py:1397(<listcomp>)\n",
      "    10560    0.002    0.000    0.002    0.000 {method 'replace' of 'str' objects}\n",
      "     2816    0.001    0.000    0.001    0.000 {built-in method posix.getpid}\n",
      "     2816    0.001    0.000    0.001    0.000 base.py:108(__init__)\n",
      "     3520    0.001    0.000    0.001    0.000 indexing.py:224(check_selection_length)\n",
      "     3520    0.001    0.000    0.001    0.000 indexing.py:61(is_pure_fancy_indexing)\n",
      "     1408    0.001    0.000    0.001    0.000 enum.py:792(value)\n",
      "     3520    0.001    0.000    0.001    0.000 core.py:428(ndim)\n",
      "     1408    0.001    0.000    0.001    0.000 {built-in method torch.is_floating_point}\n",
      "     3520    0.001    0.000    0.001    0.000 meta.py:183(_decode_dtype_descr)\n",
      "     1408    0.001    0.000    0.001    0.000 {built-in method builtins.sorted}\n",
      "     7040    0.001    0.000    0.001    0.000 {built-in method torch._C._has_torch_function}\n",
      "     7040    0.001    0.000    0.001    0.000 _compat.py:90(get_generic_base)\n",
      "     1408    0.001    0.000    0.001    0.000 functional_tensor.py:545(_cast_squeeze_in)\n",
      "    11264    0.001    0.000    0.001    0.000 {method 'append' of 'list' objects}\n",
      "     4224    0.001    0.000    0.001    0.000 {method 'is_floating_point' of 'torch._C._TensorBase' objects}\n",
      "     7040    0.001    0.000    0.001    0.000 indexing.py:839(check_fields)\n",
      "     3520    0.001    0.000    0.001    0.000 store.py:312(_path_to_prefix)\n",
      "       66    0.001    0.000    0.001    0.000 collate.py:160(<listcomp>)\n",
      "     3520    0.001    0.000    0.001    0.000 hierarchy.py:223(attrs)\n",
      "     1408    0.001    0.000    0.001    0.000 _utils.py:645(is_list_annotation)\n",
      "     3520    0.001    0.000    0.001    0.000 indexing.py:234(<genexpr>)\n",
      "     8448    0.001    0.000    0.001    0.000 nodes.py:31(_value)\n",
      "     3520    0.001    0.000    0.001    0.000 util.py:387(__init__)\n",
      "     7040    0.001    0.000    0.001    0.000 {method 'end' of 're.Match' objects}\n",
      "     3520    0.001    0.000    0.001    0.000 indexing.py:813(__init__)\n",
      "     4224    0.001    0.000    0.001    0.000 base.py:128(_invalidate_flags_cache)\n",
      "     2816    0.001    0.000    0.001    0.000 process.py:99(_check_closed)\n",
      "     2816    0.001    0.000    0.001    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
      "     3520    0.001    0.000    0.001    0.000 indexing.py:657(__init__)\n",
      "      704    0.001    0.000    0.001    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
      "     1408    0.001    0.000    0.001    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
      "     4224    0.001    0.000    0.001    0.000 base.py:344(_is_flags_root)\n",
      "     7040    0.001    0.000    0.001    0.000 {built-in method torch._C._has_torch_function_unary}\n",
      "     1408    0.001    0.000    0.001    0.000 _utils.py:655(is_tuple_annotation)\n",
      "     2816    0.001    0.000    0.001    0.000 process.py:37(current_process)\n",
      "     1408    0.001    0.000    0.001    0.000 numeric.py:1472(<listcomp>)\n",
      "     2816    0.000    0.000    0.000    0.000 process.py:189(name)\n",
      "     3061    0.000    0.000    0.000    0.000 core.py:283(_refresh_metadata)\n",
      "     2816    0.000    0.000    0.000    0.000 threading.py:1484(main_thread)\n",
      "     2816    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_variadic}\n",
      "     1408    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
      "     2816    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
      "     1408    0.000    0.000    0.000    0.000 functional_tensor.py:561(_cast_squeeze_out)\n",
      "     1408    0.000    0.000    0.000    0.000 base.py:67(__post_init__)\n",
      "       22    0.000    0.000    0.000    0.000 _ops.py:138(__call__)\n",
      "     1408    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
      "     3061    0.000    0.000    0.000    0.000 core.py:386(dtype)\n",
      "     1410    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "     2816    0.000    0.000    0.000    0.000 {built-in method _operator.index}\n",
      "       11    0.000    0.000    0.000    0.000 profiler.py:445(__enter__)\n",
      "       11    0.000    0.000    0.000    0.000 profiler.py:449(__exit__)\n",
      "     1408    0.000    0.000    0.000    0.000 numeric.py:1406(_moveaxis_dispatcher)\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method torch._ops.profiler._record_function_enter}\n",
      "       11    0.000    0.000    0.000    0.000 profiler.py:436(__init__)\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method torch.zeros}\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method torch._ops.profiler._record_function_exit}\n",
      "      704    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "       11    0.000    0.000    0.000    0.000 dataloader.py:670(_next_index)\n",
      "       12    0.000    0.000    0.000    0.000 sampler.py:234(__iter__)\n",
      "        1    0.000    0.000    0.000    0.000 dataloader.py:431(__iter__)\n",
      "        1    0.000    0.000    0.000    0.000 dataloader.py:385(_get_iterator)\n",
      "        1    0.000    0.000    0.000    0.000 dataloader.py:711(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 dataloader.py:621(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'random_' of 'torch._C._TensorBase' objects}\n",
      "       55    0.000    0.000    0.000    0.000 worker.py:83(get_worker_info)\n",
      "        1    0.000    0.000    0.000    0.000 sampler.py:75(__iter__)\n",
      "        1    0.000    0.000    0.000    0.000 real_image_dataset.py:275(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 cProfile.py:118(__exit__)\n",
      "        1    0.000    0.000    0.000    0.000 dataloader.py:77(create_fetcher)\n",
      "        1    0.000    0.000    0.000    0.000 dataloader.py:568(_get_shared_seed)\n",
      "        1    0.000    0.000    0.000    0.000 fetch.py:44(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 sampler.py:122(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 fetch.py:8(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 dataloader.py:450(_index_sampler)\n",
      "        2    0.000    0.000    0.000    0.000 dataloader.py:446(_auto_collation)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cprofile a single dataloader iteration\n",
    "import cProfile\n",
    "dataloader = DataLoader(dataset, batch_size=64, num_workers=0, shuffle=False)\n",
    "with cProfile.Profile() as pr:\n",
    "    c = 0\n",
    "    for batch in dataloader:\n",
    "        c += 1\n",
    "        if c > 10:\n",
    "            break\n",
    "       \n",
    "    \n",
    "\n",
    "pr.print_stats(sort=\"cumtime\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
