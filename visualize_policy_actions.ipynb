{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tlips/.conda/envs/robodiff/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from diffusion_policy.dataset.real_image_dataset import RealImageDataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"/home/tlips/Code/diffusion_policy/data/demo_place-cb-val\"\n",
    "dataset_path = \"/home/tlips/Code/diffusion_policy/data/demo_place-cb-val\"\n",
    "import os\n",
    "os.path.exists(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "numcodec 'imagecodecs_aec' already registered\n",
      "numcodec 'imagecodecs_apng' already registered\n",
      "numcodec 'imagecodecs_avif' already registered\n",
      "numcodec 'imagecodecs_bitorder' already registered\n",
      "numcodec 'imagecodecs_bitshuffle' already registered\n",
      "numcodec 'imagecodecs_blosc' already registered\n",
      "numcodec 'imagecodecs_blosc2' already registered\n",
      "numcodec 'imagecodecs_brotli' already registered\n",
      "numcodec 'imagecodecs_byteshuffle' already registered\n",
      "numcodec 'imagecodecs_bz2' already registered\n",
      "numcodec 'imagecodecs_cms' already registered\n",
      "numcodec 'imagecodecs_deflate' already registered\n",
      "numcodec 'imagecodecs_delta' already registered\n",
      "numcodec 'imagecodecs_float24' already registered\n",
      "numcodec 'imagecodecs_floatpred' already registered\n",
      "numcodec 'imagecodecs_gif' already registered\n",
      "numcodec 'imagecodecs_heif' already registered\n",
      "numcodec 'imagecodecs_jetraw' already registered\n",
      "numcodec 'imagecodecs_jpeg' already registered\n",
      "numcodec 'imagecodecs_jpeg2k' already registered\n",
      "numcodec 'imagecodecs_jpegls' already registered\n",
      "numcodec 'imagecodecs_jpegxl' already registered\n",
      "numcodec 'imagecodecs_jpegxr' already registered\n",
      "numcodec 'imagecodecs_lerc' already registered\n",
      "numcodec 'imagecodecs_ljpeg' already registered\n",
      "numcodec 'imagecodecs_lz4' already registered\n",
      "numcodec 'imagecodecs_lz4f' already registered\n",
      "numcodec 'imagecodecs_lzf' already registered\n",
      "numcodec 'imagecodecs_lzma' already registered\n",
      "numcodec 'imagecodecs_lzw' already registered\n",
      "numcodec 'imagecodecs_packbits' already registered\n",
      "numcodec 'imagecodecs_pglz' already registered\n",
      "numcodec 'imagecodecs_png' already registered\n",
      "numcodec 'imagecodecs_qoi' already registered\n",
      "numcodec 'imagecodecs_rcomp' already registered\n",
      "numcodec 'imagecodecs_snappy' already registered\n",
      "numcodec 'imagecodecs_spng' already registered\n",
      "numcodec 'imagecodecs_tiff' already registered\n",
      "numcodec 'imagecodecs_webp' already registered\n",
      "numcodec 'imagecodecs_xor' already registered\n",
      "numcodec 'imagecodecs_zfp' already registered\n",
      "numcodec 'imagecodecs_zlib' already registered\n",
      "numcodec 'imagecodecs_zlibng' already registered\n",
      "numcodec 'imagecodecs_zopfli' already registered\n",
      "numcodec 'imagecodecs_zstd' already registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer shape meta:\n",
      "{'obs': {'camera_0': {'shape': [3, 224, 224], 'type': 'rgb'}, 'camera_1': {'shape': [3, 224, 224], 'type': 'rgb'}, 'robot_eef_pose_6d_rot': {'shape': [9], 'type': 'low_dim'}, 'gripper_width': {'shape': [1], 'type': 'low_dim'}}, 'action': {'shape': [10]}}\n",
      "Loading lowdim data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading image data: 100%|██████████| 1834/1834 [00:04<00:00, 449.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_keys: ['camera_0', 'camera_1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_latency_steps = 0\n",
    "n_obs_steps = 2\n",
    "n_action_steps = 8\n",
    "horizon = 16\n",
    "\n",
    "shape_meta = {\n",
    "    \"obs\": {\n",
    "        \"camera_0\": {\n",
    "            \"shape\": [3, 224, 224],\n",
    "            \"type\": \"rgb\"\n",
    "        },\n",
    "        \"camera_1\": {\n",
    "            \"shape\": [3, 224, 224],\n",
    "            \"type\": \"rgb\"\n",
    "        },\n",
    "        \"robot_eef_pose_6d_rot\": {\n",
    "            \"shape\": [9],\n",
    "            \"type\": \"low_dim\"\n",
    "        },\n",
    "        \"gripper_width\": {\n",
    "            \"shape\": [1],\n",
    "            \"type\": \"low_dim\"\n",
    "        }\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"shape\": [10]\n",
    "    }\n",
    "}\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "shape_meta = OmegaConf.create(shape_meta)\n",
    "\n",
    "dataset = RealImageDataset(shape_meta=shape_meta, dataset_path=dataset_path, image_buffer_resolution=[256,256],\n",
    " horizon=horizon, pad_before=n_obs_steps-1+n_latency_steps, pad_after=n_action_steps-1, n_obs_steps=n_obs_steps, n_latency_steps=n_latency_steps, \n",
    "use_cache=False, seed=42, val_ratio=0.1, max_train_episodes=None,\n",
    " delta_action=True,\n",
    " image_transforms=[transforms.Resize(240),transforms.RandomCrop(224),transforms.ColorJitter()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera_0 torch.Size([2, 3, 224, 224])\n",
      "camera_1 torch.Size([2, 3, 224, 224])\n",
      "robot_eef_pose_6d_rot torch.Size([2, 9])\n",
      "gripper_width torch.Size([2, 1])\n",
      "action torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "x = dataset[10]\n",
    "obs = x[\"obs\"]\n",
    "action = x[\"action\"]\n",
    "for k in obs:\n",
    "    print(k, obs[k].shape)\n",
    "print(\"action\",action.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         906924 function calls (905524 primitive calls) in 0.439 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.439    0.439 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.439    0.439 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.439    0.439 3083052460.py:5(profile_dataset_access)\n",
      "      100    0.011    0.000    0.439    0.004 real_image_dataset.py:287(__getitem__)\n",
      "      100    0.000    0.000    0.182    0.002 real_image_dataset.py:279(_get_from_sampler)\n",
      "      100    0.004    0.000    0.182    0.002 sampler.py:125(sample_sequence)\n",
      "      200    0.000    0.000    0.172    0.001 transforms.py:92(__call__)\n",
      "      600    0.001    0.000    0.172    0.000 module.py:1124(_call_impl)\n",
      "      200    0.000    0.000    0.159    0.001 transforms.py:341(forward)\n",
      "      200    0.000    0.000    0.159    0.001 functional.py:363(resize)\n",
      "      200    0.001    0.000    0.158    0.001 functional_tensor.py:429(resize)\n",
      "      200    0.001    0.000    0.157    0.001 functional.py:3757(interpolate)\n",
      "      200    0.156    0.001    0.156    0.001 {built-in method torch._C._nn.upsample_bilinear2d}\n",
      "      500    0.001    0.000    0.124    0.000 core.py:648(__getitem__)\n",
      "      500    0.000    0.000    0.123    0.000 core.py:791(get_basic_selection)\n",
      "      500    0.000    0.000    0.122    0.000 core.py:951(_get_basic_selection_nd)\n",
      "      500    0.001    0.000    0.115    0.000 core.py:1219(_get_selection)\n",
      "      700    0.001    0.000    0.107    0.000 core.py:1906(_chunk_getitem)\n",
      "    77400    0.007    0.000    0.096    0.000 {built-in method builtins.isinstance}\n",
      "     3600    0.001    0.000    0.089    0.000 compat.py:13(ensure_ndarray_like)\n",
      "     3600    0.001    0.000    0.087    0.000 ndarray_like.py:54(is_ndarray_like)\n",
      "     3600    0.003    0.000    0.085    0.000 typing.py:1141(__instancecheck__)\n",
      "      700    0.027    0.000    0.083    0.000 core.py:1823(_process_chunk)\n",
      "     1300    0.001    0.000    0.066    0.000 compat.py:126(ensure_contiguous_ndarray)\n",
      "     2300    0.001    0.000    0.059    0.000 compat.py:48(ensure_ndarray)\n",
      "     7200    0.040    0.000    0.056    0.000 typing.py:1065(_get_protocol_attrs)\n",
      "      500    0.000    0.000    0.047    0.000 replay_buffer.py:415(__getitem__)\n",
      "      500    0.001    0.000    0.047    0.000 hierarchy.py:388(__getitem__)\n",
      "      500    0.002    0.000    0.041    0.000 core.py:154(__init__)\n",
      "     3600    0.002    0.000    0.036    0.000 typing.py:1082(_is_callable_members_only)\n",
      "      500    0.000    0.000    0.034    0.000 core.py:217(_load_metadata)\n",
      "      500    0.001    0.000    0.034    0.000 core.py:226(_load_metadata_nosync)\n",
      "     1300    0.001    0.000    0.033    0.000 compat.py:70(ensure_contiguous_ndarray_like)\n",
      "      500    0.001    0.000    0.032    0.000 meta.py:108(decode_array_metadata)\n",
      "      500    0.000    0.000    0.029    0.000 meta.py:90(parse_metadata)\n",
      "     8600    0.004    0.000    0.029    0.000 {built-in method builtins.all}\n",
      "      200    0.000    0.000    0.029    0.000 listconfig.py:512(__eq__)\n",
      "      500    0.000    0.000    0.029    0.000 util.py:54(json_loads)\n",
      "      500    0.000    0.000    0.026    0.000 compat.py:178(ensure_text)\n",
      "      200    0.000    0.000    0.022    0.000 listconfig.py:46(__init__)\n",
      "      200    0.000    0.000    0.020    0.000 listconfig.py:610(_set_value)\n",
      "      200    0.001    0.000    0.020    0.000 listconfig.py:620(_set_value_impl)\n",
      "    38400    0.009    0.000    0.018    0.000 typing.py:1149(<genexpr>)\n",
      "      500    0.018    0.000    0.018    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "      200    0.000    0.000    0.015    0.000 listconfig.py:293(append)\n",
      "      200    0.001    0.000    0.015    0.000 basecontainer.py:522(_set_item_impl)\n",
      "      200    0.002    0.000    0.012    0.000 real_image_dataset.py:35(position_and_rot6d_to_se3)\n",
      "   188500    0.010    0.000    0.010    0.000 {method 'startswith' of 'str' objects}\n",
      "      200    0.002    0.000    0.008    0.000 rotation_conversions.py:556(rotation_6d_to_matrix)\n",
      "      400    0.000    0.000    0.007    0.000 dictconfig.py:361(__getitem__)\n",
      "      600    0.006    0.000    0.007    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "      200    0.006    0.000    0.007    0.000 transforms.py:1225(forward)\n",
      "      400    0.000    0.000    0.007    0.000 dictconfig.py:438(_get_impl)\n",
      "      200    0.000    0.000    0.007    0.000 listconfig.py:662(_list_eq)\n",
      "      300    0.000    0.000    0.007    0.000 core.py:2142(_decode_chunk)\n",
      "      200    0.000    0.000    0.007    0.000 basecontainer.py:620(_wrap_value_and_set)\n",
      "      500    0.002    0.000    0.007    0.000 indexing.py:328(__init__)\n",
      "     1200    0.002    0.000    0.007    0.000 indexing.py:356(__iter__)\n",
      "      400    0.000    0.000    0.007    0.000 numeric.py:289(full)\n",
      "      200    0.000    0.000    0.007    0.000 omegaconf.py:1076(_maybe_wrap)\n",
      "      200    0.000    0.000    0.006    0.000 omegaconf.py:979(_node_wrap)\n",
      "    71900    0.006    0.000    0.006    0.000 {built-in method builtins.getattr}\n",
      "      400    0.000    0.000    0.006    0.000 <__array_function__ internals>:177(copyto)\n",
      "    21400    0.002    0.000    0.005    0.000 abc.py:117(__instancecheck__)\n",
      "      700    0.000    0.000    0.005    0.000 indexing.py:312(is_contiguous_selection)\n",
      "     3200    0.002    0.000    0.005    0.000 _utils.py:507(_is_missing_value)\n",
      "      200    0.001    0.000    0.005    0.000 listconfig.py:90(_validate_set)\n",
      "      200    0.000    0.000    0.005    0.000 transforms.py:655(forward)\n",
      "      400    0.000    0.000    0.005    0.000 basecontainer.py:86(_resolve_with_default)\n",
      "      400    0.001    0.000    0.004    0.000 functional.py:4598(normalize)\n",
      "      200    0.000    0.000    0.004    0.000 nodes.py:128(__init__)\n",
      "     2900    0.001    0.000    0.004    0.000 indexing.py:314(<genexpr>)\n",
      "     1500    0.003    0.000    0.004    0.000 util.py:309(normalize_storage_path)\n",
      "   100800    0.004    0.000    0.004    0.000 {method 'add' of 'set' objects}\n",
      "      200    0.000    0.000    0.004    0.000 basecontainer.py:637(_item_eq)\n",
      "      200    0.000    0.000    0.004    0.000 nodes.py:24(__init__)\n",
      "    45700    0.004    0.000    0.004    0.000 {built-in method builtins.hasattr}\n",
      "      800    0.000    0.000    0.004    0.000 basecontainer.py:64(_get_child)\n",
      "      800    0.000    0.000    0.003    0.000 {built-in method builtins.next}\n",
      "     3000    0.002    0.000    0.003    0.000 indexing.py:178(__iter__)\n",
      "      800    0.001    0.000    0.003    0.000 basecontainer.py:179(__len__)\n",
      "14900/14300    0.001    0.000    0.003    0.000 {built-in method builtins.len}\n",
      "    10800    0.002    0.000    0.003    0.000 typing.py:1084(<genexpr>)\n",
      "     1800    0.000    0.000    0.003    0.000 base.py:298(_is_missing)\n",
      "     1000    0.000    0.000    0.003    0.000 _utils.py:448(is_structured_config)\n",
      "      500    0.000    0.000    0.003    0.000 __init__.py:299(loads)\n",
      "     2600    0.001    0.000    0.003    0.000 _utils.py:566(_is_interpolation)\n",
      "     1000    0.000    0.000    0.003    0.000 _utils.py:540(get_value_kind)\n",
      "      800    0.001    0.000    0.003    0.000 _utils.py:747(_get_value)\n",
      "      500    0.000    0.000    0.003    0.000 storage.py:101(contains_array)\n",
      "     2200    0.001    0.000    0.003    0.000 indexing.py:36(is_integer_array)\n",
      "    21400    0.003    0.000    0.003    0.000 {built-in method _abc._abc_instancecheck}\n",
      "      800    0.001    0.000    0.003    0.000 listconfig.py:402(_get_node)\n",
      "      500    0.001    0.000    0.003    0.000 decoder.py:332(decode)\n",
      "      800    0.001    0.000    0.002    0.000 base.py:152(_set_flag)\n",
      "     6800    0.002    0.000    0.002    0.000 <frozen importlib._bootstrap>:1033(_handle_fromlist)\n",
      "      400    0.000    0.000    0.002    0.000 _tensor.py:527(norm)\n",
      "     1400    0.001    0.000    0.002    0.000 indexing.py:165(__init__)\n",
      "      400    0.000    0.000    0.002    0.000 omegaconf.py:936(flag_override)\n",
      "      400    0.001    0.000    0.002    0.000 functional.py:1345(norm)\n",
      "      500    0.000    0.000    0.002    0.000 hierarchy.py:360(_item_path)\n",
      "      200    0.000    0.000    0.002    0.000 transforms.py:620(get_params)\n",
      "      400    0.000    0.000    0.002    0.000 contextlib.py:114(__enter__)\n",
      "     2200    0.001    0.000    0.002    0.000 numeric.py:1873(isscalar)\n",
      "1800/1400    0.001    0.000    0.002    0.000 base.py:189(_get_flag)\n",
      "      200    0.000    0.000    0.002    0.000 nodes.py:34(_set_value)\n",
      "     1200    0.001    0.000    0.002    0.000 storage.py:816(__getitem__)\n",
      "      700    0.001    0.000    0.002    0.000 util.py:200(is_total_slice)\n",
      "      400    0.000    0.000    0.002    0.000 contextlib.py:123(__exit__)\n",
      "      500    0.000    0.000    0.002    0.000 storage.py:158(normalize_store_arg)\n",
      "    40800    0.002    0.000    0.002    0.000 {built-in method builtins.callable}\n",
      "      100    0.001    0.000    0.002    0.000 real_image_dataset.py:51(se3_to_position_and_rot6d)\n",
      "     1700    0.001    0.000    0.002    0.000 storage.py:789(_get_parent)\n",
      "      400    0.000    0.000    0.002    0.000 functional.py:62(get_dimensions)\n",
      "      200    0.002    0.000    0.002    0.000 {method 'repeat' of 'torch._C._TensorBase' objects}\n",
      "      400    0.000    0.000    0.002    0.000 dictconfig.py:455(_get_node)\n",
      "      400    0.000    0.000    0.002    0.000 base.py:706(_maybe_resolve_interpolation)\n",
      "      200    0.000    0.000    0.002    0.000 <__array_function__ internals>:177(moveaxis)\n",
      "      500    0.000    0.000    0.001    0.000 storage.py:131(_normalize_store_arg_v2)\n",
      "      200    0.000    0.000    0.001    0.000 nodes.py:56(validate_and_convert)\n",
      "     1000    0.001    0.000    0.001    0.000 _utils.py:440(is_attr_class)\n",
      "     1000    0.001    0.000    0.001    0.000 _utils.py:432(is_dataclass)\n",
      "      500    0.001    0.000    0.001    0.000 decoder.py:343(raw_decode)\n",
      "      200    0.000    0.000    0.001    0.000 functional.py:484(crop)\n",
      "     1000    0.001    0.000    0.001    0.000 store.py:92(_ensure_store)\n",
      "      500    0.001    0.000    0.001    0.000 indexing.py:229(replace_ellipsis)\n",
      "      100    0.001    0.000    0.001    0.000 {built-in method torch.matmul}\n",
      "      200    0.000    0.000    0.001    0.000 numeric.py:1410(moveaxis)\n",
      "      200    0.000    0.000    0.001    0.000 omegaconf.py:632(get_type)\n",
      "     6000    0.001    0.000    0.001    0.000 indexing.py:159(ceildiv)\n",
      "      200    0.000    0.000    0.001    0.000 omegaconf.py:888(_get_obj_type)\n",
      "      700    0.001    0.000    0.001    0.000 core.py:478(_cdata_shape)\n",
      "     4000    0.001    0.000    0.001    0.000 {built-in method numpy.array}\n",
      "      400    0.001    0.000    0.001    0.000 {built-in method torch.norm}\n",
      "      800    0.000    0.000    0.001    0.000 basecontainer.py:694(_is_interpolation)\n",
      "      800    0.001    0.000    0.001    0.000 utils.py:543(_log_api_usage_once)\n",
      "      400    0.001    0.000    0.001    0.000 base.py:773(_invalidate_flags_cache)\n",
      "     3200    0.001    0.000    0.001    0.000 <frozen importlib._bootstrap>:398(parent)\n",
      "      500    0.000    0.000    0.001    0.000 attrs.py:27(__init__)\n",
      " 1200/800    0.000    0.000    0.001    0.000 base.py:201(_get_flag_no_cache)\n",
      "      400    0.000    0.000    0.001    0.000 omegaconf.py:956(read_write)\n",
      "      100    0.000    0.000    0.001    0.000 rotation_conversions.py:580(matrix_to_rotation_6d)\n",
      "      400    0.000    0.000    0.001    0.000 dictconfig.py:149(_validate_get)\n",
      "      400    0.001    0.000    0.001    0.000 {built-in method torch.randint}\n",
      "     1500    0.000    0.000    0.001    0.000 {built-in method builtins.any}\n",
      "     1400    0.000    0.000    0.001    0.000 indexing.py:18(is_integer)\n",
      "      800    0.001    0.000    0.001    0.000 functional_tensor.py:24(get_dimensions)\n",
      "      100    0.001    0.000    0.001    0.000 {built-in method torch._C._linalg.linalg_inv}\n",
      "     2600    0.001    0.000    0.001    0.000 util.py:214(<genexpr>)\n",
      "      200    0.000    0.000    0.001    0.000 _utils.py:596(_is_special)\n",
      "      200    0.000    0.000    0.001    0.000 _tensor.py:713(__iter__)\n",
      "      200    0.001    0.000    0.001    0.000 functional_tensor.py:132(crop)\n",
      "      700    0.000    0.000    0.001    0.000 core.py:2131(_chunk_key)\n",
      "      500    0.000    0.000    0.001    0.000 storage.py:842(__contains__)\n",
      "      200    0.001    0.000    0.001    0.000 {built-in method torch.stack}\n",
      "      200    0.000    0.000    0.001    0.000 nodes.py:143(_validate_and_convert_impl)\n",
      "     2200    0.000    0.000    0.001    0.000 indexing.py:304(is_contiguous_slice)\n",
      "      400    0.000    0.000    0.001    0.000 omegaconf.py:494(set_readonly)\n",
      "     1900    0.001    0.000    0.001    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "      500    0.001    0.000    0.001    0.000 {built-in method torch.from_numpy}\n",
      "      200    0.001    0.000    0.001    0.000 {built-in method torch.cat}\n",
      "      500    0.000    0.000    0.001    0.000 meta.py:215(decode_fill_value)\n",
      "      200    0.000    0.000    0.001    0.000 transforms.py:1193(get_params)\n",
      "     7200    0.001    0.000    0.001    0.000 {method 'keys' of 'mappingproxy' objects}\n",
      "      200    0.000    0.000    0.001    0.000 basecontainer.py:560(get_target_type_hint)\n",
      "      400    0.000    0.000    0.001    0.000 numeric.py:1347(normalize_axis_tuple)\n",
      "     3600    0.000    0.000    0.001    0.000 _utils.py:515(_is_missing_literal)\n",
      "      400    0.001    0.000    0.001    0.000 {method 'clamp_min' of 'torch._C._TensorBase' objects}\n",
      "     2900    0.000    0.000    0.001    0.000 core.py:483(<genexpr>)\n",
      "      400    0.000    0.000    0.001    0.000 _utils.py:620(is_primitive_list)\n",
      "      100    0.000    0.000    0.001    0.000 pytorch_util.py:6(dict_apply)\n",
      "      200    0.001    0.000    0.001    0.000 {built-in method torch.eye}\n",
      "      200    0.001    0.000    0.001    0.000 {method 'unbind' of 'torch._C._TensorBase' objects}\n",
      "      200    0.000    0.000    0.001    0.000 <string>:2(__init__)\n",
      "      200    0.001    0.000    0.001    0.000 {built-in method torch.randperm}\n",
      "      400    0.000    0.000    0.001    0.000 nodes.py:113(_is_interpolation)\n",
      "     3600    0.000    0.000    0.001    0.000 indexing.py:300(is_slice)\n",
      "      900    0.001    0.000    0.001    0.000 {built-in method numpy.empty}\n",
      "     1800    0.000    0.000    0.001    0.000 base.py:304(_is_none)\n",
      "      200    0.001    0.000    0.001    0.000 {method 'sum' of 'torch._C._TensorBase' objects}\n",
      "      500    0.000    0.000    0.001    0.000 meta.py:191(decode_dtype)\n",
      "      200    0.001    0.000    0.001    0.000 {built-in method torch.cross}\n",
      "     1000    0.001    0.000    0.001    0.000 {method 'match' of 're.Pattern' objects}\n",
      "      200    0.000    0.000    0.001    0.000 registry.py:23(get_codec)\n",
      "     8200    0.000    0.000    0.000    0.000 {built-in method math.ceil}\n",
      "     1000    0.000    0.000    0.000    0.000 dataclasses.py:1047(is_dataclass)\n",
      "     4800    0.000    0.000    0.000    0.000 basecontainer.py:705(_value)\n",
      "      400    0.000    0.000    0.000    0.000 contextlib.py:261(helper)\n",
      "      400    0.000    0.000    0.000    0.000 dictconfig.py:144(_is_typed)\n",
      "     7300    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "     5500    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "     1000    0.000    0.000    0.000    0.000 _funcs.py:326(has)\n",
      "     3200    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
      "     4000    0.000    0.000    0.000    0.000 util.py:345(<genexpr>)\n",
      "      800    0.000    0.000    0.000    0.000 _trace.py:1008(is_tracing)\n",
      "      400    0.000    0.000    0.000    0.000 _utils.py:520(_is_none)\n",
      "      700    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n",
      "      800    0.000    0.000    0.000    0.000 _utils.py:212(_resolve_optional)\n",
      "      100    0.000    0.000    0.000    0.000 {method 'clone' of 'torch._C._TensorBase' objects}\n",
      "      400    0.000    0.000    0.000    0.000 {method 'unsqueeze' of 'torch._C._TensorBase' objects}\n",
      "      500    0.000    0.000    0.000    0.000 {built-in method _codecs.decode}\n",
      "     1200    0.000    0.000    0.000    0.000 functional_tensor.py:13(_assert_image_tensor)\n",
      "      400    0.000    0.000    0.000    0.000 {method 'expand_as' of 'torch._C._TensorBase' objects}\n",
      "      400    0.000    0.000    0.000    0.000 process.py:234(ident)\n",
      "      400    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)\n",
      "     1900    0.000    0.000    0.000    0.000 indexing.py:352(<genexpr>)\n",
      "     2000    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "      200    0.000    0.000    0.000    0.000 base.py:88(__post_init__)\n",
      "     1000    0.000    0.000    0.000    0.000 store.py:423(_prefix_to_array_key)\n",
      "      600    0.000    0.000    0.000    0.000 _utils.py:626(is_primitive_dict)\n",
      "     3200    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
      "      800    0.000    0.000    0.000    0.000 {built-in method torch._C._get_tracing_state}\n",
      "      400    0.000    0.000    0.000    0.000 _utils.py:677(is_dict)\n",
      "      400    0.000    0.000    0.000    0.000 dictconfig.py:274(_validate_and_normalize_key)\n",
      "     1000    0.000    0.000    0.000    0.000 _utils.py:205(is_union_annotation)\n",
      "      200    0.000    0.000    0.000    0.000 basecontainer.py:59(__init__)\n",
      "     2300    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x55cf220a9380}\n",
      "     2900    0.000    0.000    0.000    0.000 indexing.py:359(<genexpr>)\n",
      "      200    0.000    0.000    0.000    0.000 abc.py:97(from_config)\n",
      "      800    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}\n",
      "      200    0.000    0.000    0.000    0.000 omegaconf.py:947(<listcomp>)\n",
      "     1200    0.000    0.000    0.000    0.000 indexing.py:274(ensure_tuple)\n",
      "      600    0.000    0.000    0.000    0.000 base.py:131(_get_parent)\n",
      "     2900    0.000    0.000    0.000    0.000 indexing.py:361(<genexpr>)\n",
      "      800    0.000    0.000    0.000    0.000 _utils.py:463(get_type_of)\n",
      "     1400    0.000    0.000    0.000    0.000 {method 'indices' of 'slice' objects}\n",
      "      500    0.000    0.000    0.000    0.000 {built-in method builtins.sum}\n",
      "      500    0.000    0.000    0.000    0.000 indexing.py:874(pop_fields)\n",
      "      600    0.000    0.000    0.000    0.000 base.py:181(_get_node_flag)\n",
      "     3000    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
      "     1400    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "     2900    0.000    0.000    0.000    0.000 indexing.py:360(<genexpr>)\n",
      "     1000    0.000    0.000    0.000    0.000 listconfig.py:84(_validate_get)\n",
      "      700    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
      "      400    0.000    0.000    0.000    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
      "      100    0.000    0.000    0.000    0.000 {method 'reshape' of 'torch._C._TensorBase' objects}\n",
      "      200    0.000    0.000    0.000    0.000 _utils.py:725(is_valid_value_annotation)\n",
      "     1200    0.000    0.000    0.000    0.000 functional_tensor.py:9(_is_tensor_a_torch_image)\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method numpy.zeros}\n",
      "      200    0.000    0.000    0.000    0.000 base.py:364(__init__)\n",
      "      400    0.000    0.000    0.000    0.000 threading.py:1358(current_thread)\n",
      "      300    0.000    0.000    0.000    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
      "      405    0.000    0.000    0.000    0.000 core.py:367(shape)\n",
      "      500    0.000    0.000    0.000    0.000 indexing.py:224(check_selection_length)\n",
      "      400    0.000    0.000    0.000    0.000 numeric.py:1397(<listcomp>)\n",
      "     1200    0.000    0.000    0.000    0.000 core.py:359(chunk_store)\n",
      "      500    0.000    0.000    0.000    0.000 store.py:447(_prefix_to_attrs_key)\n",
      "      400    0.000    0.000    0.000    0.000 dictconfig.py:277(_s_validate_and_normalize_key)\n",
      "      100    0.000    0.000    0.000    0.000 {method 'squeeze' of 'torch._C._TensorBase' objects}\n",
      "     1500    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
      "      500    0.000    0.000    0.000    0.000 indexing.py:61(is_pure_fancy_indexing)\n",
      "      500    0.000    0.000    0.000    0.000 core.py:428(ndim)\n",
      "      400    0.000    0.000    0.000    0.000 base.py:108(__init__)\n",
      "      400    0.000    0.000    0.000    0.000 _VF.py:25(__getattr__)\n",
      "      400    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "     1600    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "      500    0.000    0.000    0.000    0.000 meta.py:183(_decode_dtype_descr)\n",
      "      200    0.000    0.000    0.000    0.000 _utils.py:631(is_dict_annotation)\n",
      "      400    0.000    0.000    0.000    0.000 base.py:65(<lambda>)\n",
      "     1000    0.000    0.000    0.000    0.000 _compat.py:90(get_generic_base)\n",
      "     1600    0.000    0.000    0.000    0.000 _jit_internal.py:958(is_scripting)\n",
      "      800    0.000    0.000    0.000    0.000 {built-in method torch._C._is_tracing}\n",
      "     1000    0.000    0.000    0.000    0.000 indexing.py:839(check_fields)\n",
      "      200    0.000    0.000    0.000    0.000 types.py:171(__get__)\n",
      "      500    0.000    0.000    0.000    0.000 util.py:387(__init__)\n",
      "      600    0.000    0.000    0.000    0.000 base.py:128(_invalidate_flags_cache)\n",
      "     1200    0.000    0.000    0.000    0.000 nodes.py:31(_value)\n",
      "      500    0.000    0.000    0.000    0.000 store.py:312(_path_to_prefix)\n",
      "     1000    0.000    0.000    0.000    0.000 {method 'end' of 're.Match' objects}\n",
      "      200    0.000    0.000    0.000    0.000 functional_tensor.py:545(_cast_squeeze_in)\n",
      "      200    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "      200    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
      "     1000    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}\n",
      "      500    0.000    0.000    0.000    0.000 indexing.py:234(<genexpr>)\n",
      "      500    0.000    0.000    0.000    0.000 indexing.py:657(__init__)\n",
      "      200    0.000    0.000    0.000    0.000 _utils.py:645(is_list_annotation)\n",
      "      400    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
      "      500    0.000    0.000    0.000    0.000 indexing.py:813(__init__)\n",
      "      200    0.000    0.000    0.000    0.000 numeric.py:1472(<listcomp>)\n",
      "      600    0.000    0.000    0.000    0.000 base.py:344(_is_flags_root)\n",
      "      500    0.000    0.000    0.000    0.000 hierarchy.py:223(attrs)\n",
      "      400    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_variadic}\n",
      "      400    0.000    0.000    0.000    0.000 process.py:99(_check_closed)\n",
      "      200    0.000    0.000    0.000    0.000 _utils.py:655(is_tuple_annotation)\n",
      "      405    0.000    0.000    0.000    0.000 core.py:283(_refresh_metadata)\n",
      "      400    0.000    0.000    0.000    0.000 multiarray.py:1071(copyto)\n",
      "      100    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
      "      400    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
      "      200    0.000    0.000    0.000    0.000 base.py:67(__post_init__)\n",
      "      400    0.000    0.000    0.000    0.000 process.py:189(name)\n",
      "      400    0.000    0.000    0.000    0.000 process.py:37(current_process)\n",
      "      405    0.000    0.000    0.000    0.000 core.py:386(dtype)\n",
      "      400    0.000    0.000    0.000    0.000 {built-in method _operator.index}\n",
      "      200    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
      "      400    0.000    0.000    0.000    0.000 threading.py:1484(main_thread)\n",
      "      200    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
      "      200    0.000    0.000    0.000    0.000 enum.py:792(value)\n",
      "      200    0.000    0.000    0.000    0.000 functional_tensor.py:561(_cast_squeeze_out)\n",
      "      200    0.000    0.000    0.000    0.000 numeric.py:1406(_moveaxis_dispatcher)\n",
      "      200    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "      100    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# profile dataset access time using python cprofiler\n",
    "\n",
    "import cProfile\n",
    "\n",
    "def profile_dataset_access():\n",
    "    for i in range(100):\n",
    "        dataset[i]\n",
    "        \n",
    "cProfile.run('profile_dataset_access()', sort='cumtime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "/\n",
       " ├── data\n",
       " │   ├── action (917, 10) float64\n",
       " │   ├── camera_0 (917, 256, 256, 3) uint8\n",
       " │   ├── camera_1 (917, 256, 256, 3) uint8\n",
       " │   ├── gripper_width (917, 1) float64\n",
       " │   └── robot_eef_pose_6d_rot (917, 9) float64\n",
       " └── meta\n",
       "     └── episode_ends (7,) int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "dataset.replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_keys: ['camera_0', 'camera_1']\n",
      "738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "/\n",
       " ├── data\n",
       " │   ├── action (917, 10) float64\n",
       " │   ├── camera_0 (917, 240, 320, 3) uint8\n",
       " │   ├── camera_1 (917, 240, 320, 3) uint8\n",
       " │   ├── gripper_width (917, 1) float64\n",
       " │   └── robot_eef_pose_6d_rot (917, 9) float64\n",
       " └── meta\n",
       "     └── episode_ends (7,) int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = dataset.get_validation_dataset()\n",
    "print(len(dataset))\n",
    "dataset.replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile dataloader to see where time is spent\n",
    "\n",
    "import torch.autograd.profiler as profiler\n",
    "with profiler.profile(record_shapes=True, profile_memory=True, use_cuda=False) as prof:\n",
    "    for i in range(10):\n",
    "        b = next(iter(dataloader))\n",
    "        print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))\n",
    "prof.export_chrome_trace(\"dataloader_trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/home/tlips/Code/diffusion_policy/data/outputs/2025.02.06/18.44.43_train_diffusion_unet_image_real_image/checkpoints/epoch=0550-train_loss=0.001.ckpt\"\n",
    "\n",
    "checkpoint_path = \"/home/tlips/Code/diffusion_policy/data/outputs/2025.02.12/22.37.02_train_diffusion_unet_image_real_image/checkpoints/epoch=0100-train_loss=0.005.ckpt\"\n",
    "omaga_config_path =\"/home/tlips/Code/diffusion_policy/data/outputs/2025.02.12/22.37.02_train_diffusion_unet_image_real_image/.hydra/config.yaml\"\n",
    "\n",
    "\n",
    "import dill\n",
    "import torch\n",
    "import hydra\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "import numpy as np\n",
    "payload = torch.load(open(checkpoint_path, 'rb'), pickle_module=dill)\n",
    "cfg = payload['cfg']\n",
    "cls = hydra.utils.get_class(cfg._target_)\n",
    "workspace = cls(cfg)\n",
    "workspace: BaseWorkspace\n",
    "workspace.load_payload(payload, exclude_keys=None, include_keys=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = workspace.model\n",
    "device = \"cuda:0\"\n",
    "policy.eval().to(device)\n",
    "\n",
    "# set inference params\n",
    "policy.num_inference_steps = 16 # DDIM inference iterations\n",
    "policy.n_action_steps = policy.horizon - policy.n_obs_steps + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = 913.7\n",
    "fy = 912.1\n",
    "cx = 644.4\n",
    "cy = 369.8\n",
    "\n",
    "intrinsics = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "\n",
    "resolution = (320,240)\n",
    "orig_resolution = (1280, 720)\n",
    "#  scale the intrinsics to the resolution of the images\n",
    "intrinsics[0] *= resolution[0] / orig_resolution[0]\n",
    "intrinsics[1] *= resolution[1] / orig_resolution[1]\n",
    "print(intrinsics)\n",
    "\n",
    "### extrinsics\n",
    "# measured using airo-mono\n",
    "\n",
    "\n",
    "\n",
    "extrinsics = np.eye(4)\n",
    "extrinsics[:3, 3] = np.array([0.5477535484731196, -0.4629795496486995, 0.488704467852322])\n",
    "rot_euler = np.array([-2.4050469949280773, -0.05730815549308965, 1.5395790079348801])\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "r = R.from_euler('xyz', rot_euler)\n",
    "extrinsics[:3, :3] = r.as_matrix()\n",
    "print(extrinsics)\n",
    "\n",
    "def project_action_to_pixel_space(action):\n",
    "    eef_pose = np.eye(4)\n",
    "    # rotate 180 degrees around y\n",
    "    # tcp_pos[:3, :3] = np.array([[1, 0, 0], [0, -1, 0], [0, 0, -1]])\n",
    "\n",
    "    # 3D actions: [x,y,z, rxx,rxy,rxz, ryx,ryy,ryz, gripper_width]\n",
    "    # do GS orthonormalization\n",
    "    rx = action[3:6]\n",
    "    ry = action[6:9]\n",
    "    rx = rx / np.linalg.norm(rx)\n",
    "    ry = (ry - np.dot(ry, rx) * rx) \n",
    "    ry = ry / np.linalg.norm(ry)\n",
    "    rz = np.cross(rx, ry)\n",
    "    eef_pose[:3, :3] = np.column_stack((rx, ry, rz))\n",
    "\n",
    "    # translate to action\n",
    "    eef_pose[:3, 3] = action[:3]\n",
    "    \n",
    "    # offset tcp z\n",
    "    tcp_in_eef = np.eye(4)\n",
    "    tcp_in_eef[2, 3] = 0.17\n",
    "    tcp_in_base = eef_pose @ tcp_in_eef\n",
    "\n",
    "    tcp_pose_in_camera = np.linalg.inv(extrinsics) @ tcp_in_base\n",
    "    tcp_pixels = intrinsics @ tcp_pose_in_camera[:3, 3]\n",
    "    tcp_pixels = tcp_pixels / tcp_pixels[2]\n",
    "    return tcp_pixels[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from diffusion_policy.real_world.real_inference_util import get_real_obs_dict\n",
    "def visualize_inference(obs, policy):\n",
    "    gt_actions = obs[\"action\"]\n",
    "    obs = obs[\"obs\"]\n",
    "    # change the images to numpy array\n",
    "    obs[\"camera_0\"] = obs[\"camera_0\"].permute(0, 2, 3, 1).numpy()\n",
    "    obs[\"camera_1\"] = obs[\"camera_1\"].permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "    obs_dict_np = get_real_obs_dict(\n",
    "        env_obs=obs, shape_meta=cfg.task.shape_meta)\n",
    "    for k, v in obs_dict_np.items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            obs_dict_np[k] = torch.from_numpy(v)\n",
    "        obs_dict_np[k] = obs_dict_np[k].unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "    # predict actions\n",
    "    actions = policy.predict_action(obs_dict_np)\n",
    "    actions = actions[\"action\"].detach().cpu().numpy()[0]\n",
    "    # project actions next to observations:\n",
    "    plt.plot(actions,label=[\"x^\",\"y^\",\"z^\",\"rxx^\",\"rxy^\",\"rxz^\",\"ryx^\",\"ryy^\",\"ryz^\",\"g^\"], alpha=0.5)\n",
    "    plt.plot(gt_actions, label=[\"x\",\"y\",\"z\",\"rxx\",\"rxy\",\"rxz\",\"ryx\",\"ryy\",\"ryz\",\"g\"], alpha=0.5, linestyle=\"--\")\n",
    "    # give each line a label: x,y,z,rxx,rxy,rzx,ryx,ryy,ryz,gripper_width\n",
    "    \n",
    "    # set legend bottom \n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), shadow=True, ncol=2)\n",
    "    plt.show()\n",
    "    # project actions into pixel space\n",
    "\n",
    "    pix_actions = []\n",
    "    for action in actions:\n",
    "        pix_action = project_action_to_pixel_space(action)\n",
    "        pix_actions.append(pix_action)\n",
    "\n",
    "    pix_actions = np.array(pix_actions)\n",
    "\n",
    "    pix_gt_actions = []\n",
    "    for action in gt_actions:\n",
    "        pix_action = project_action_to_pixel_space(action)\n",
    "        pix_gt_actions.append(pix_action)\n",
    "    pix_gt_actions = np.array(pix_gt_actions)\n",
    "\n",
    "    # visualize \n",
    "\n",
    "    img = obs[\"camera_0\"][0]\n",
    "    # display gt actions and predicted actions side by side\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].scatter(pix_gt_actions[..., 0], pix_gt_actions[..., 1], c=range(len(pix_gt_actions)), cmap=\"viridis\")\n",
    "    ax[0].legend([\"GT\"])\n",
    "    ax[1].imshow(img)\n",
    "    ax[1].scatter(pix_actions[..., 0], pix_actions[..., 1], c=range(len(pix_actions)), cmap=\"viridis\")\n",
    "    ax[1].legend([\"Predicted\"])\n",
    "    plt.show()\n",
    "    # return np array imgae of the matplotlib figure \n",
    "    fig.canvas.draw()\n",
    "    img = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 20 random samples and visualize\n",
    "import random\n",
    "imgs = []\n",
    "for i in range(2):\n",
    "    idx = random.randint(0, len(dataset))\n",
    "    obs = dataset[idx]\n",
    "    img = visualize_inference(obs, policy)\n",
    "    imgs.append(img)\n",
    "\n",
    "# store as one big image\n",
    "import cv2\n",
    "img = cv2.vconcat(imgs)\n",
    "# convert to bgr\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGRA)\n",
    "cv2.imwrite(\"inference_results.png\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "for i in range(idx,idx+100,10):\n",
    "    obs = dataset[i]\n",
    "    img = visualize_inference(obs, policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
